\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{semantic}
\usepackage{verbatim}
\usepackage{stmaryrd}
\usepackage{color}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{breakurl}

\begin{document}

\title{The F\# Computation Expressions Zoo}
\author{Tomas Petricek\inst{1} \and Don Syme\inst{2}}
\institute{University of Cambridge, UK\and Microsoft Research Cambridge, UK \\
\textsf{tp322@cam.ac.uk}, \textsf{dsyme@microsoft.com}}


\maketitle

% ==================================================================================================

\newcommand{\sep}[0]{\; | \;}
\newcommand{\kvd}[1]{\textnormal{\bfseries\sffamily #1}}
\newcommand{\plc}[1]{\textnormal{\emph{#1}}}
\newcommand{\ident}[1]{\textnormal{\sffamily #1}}

\newcommand{\cexpr}{\plc{cexpr}}
\newcommand{\expr}{\plc{expr}}
\newcommand{\binds}{\plc{binds}}
\newcommand{\pat}{\plc{pat}}

% Computation types
\newcommand{\mtyp}[1]{M #1}
\newcommand{\ntyp}[1]{N #1}
\newcommand{\ltyp}[1]{L #1}
\newcommand{\dtyp}[1]{D #1}

% Typing rules etc.
\newcommand{\tya}[2]{#1\hspace{-0.15em}:\hspace{-0.15em}#2}
\newcommand{\cvdash}{\Vdash_\sigma}
\newcommand{\bvdash}{\vartriangleright_\sigma}
\newcommand{\unit}{\ident{unit}}

% Translation rules
\newcommand{\tsl}[1]{|[ \,#1\, |]_m}
\newcommand{\tsb}[1]{\langle\hspace{-0.25em}\langle #1 \rangle\hspace{-0.25em}\rangle_m}
\newcommand{\tsv}[1]{\langle #1 \rangle}

% ==================================================================================================

\begin{abstract}
Many computations can be structured using abstract types such as monoids, monad transformers or 
applicative functors. Functional programmers use those abstractions directly while main-stream
languages often integrate concrete instances as language features -- e.g. generators in Python
or asynchronous computations in C\# 5.0. The question is, is there a sweet spot between a series of
convenient, hardwired language features, and an inconvenient but flexible set of libraries?

\quad F\# \emph{computation expressions} answer this question in the affirmative. Unlike 
the ``do'' notation in Haskell, computation expressions are not tied to a single kind of abstraction.
They support a wide range of computations, depending on what operations are available. They also 
provide greater syntactic flexibility leading to a more intuitive syntax, without resorting to 
full macro-based meta-programming. 

\quad We show that computation expressions can structure well-known computations including monoidal 
list comprehensions, monadic parsers, applicative formlets and asynchronous sequences based on the 
list monad transformer. We also present typing rules for computation expressions that are capable of 
capturing all these applications. 

\end{abstract}

% ==================================================================================================

\section{Introduction}
Computations with non-standard aspects like non-determinism, effects, asynchronicity or their
combinations can be captured using a variety of abstract computation types. In Haskell, we write 
such computations using a mix of combinators and syntactic extensions like monad comprehensions 
\cite{monad-compre} and ``do'' notation. Languages such as Python and C\# emphasize the syntax 
and provide single-purpose support e.g. for asynchrony \cite{cs-async} and list generators. 

Using such abstractions can be made simpler and more intuitive 
if we employ a general syntactic machinery. F\# computation expressions provide
\emph{uniform} syntax that supports monoids, monads \cite{monads-fp}, monad transformers 
\cite{monad-transformers} and applicative functors \cite{applicative}. They reuse familiar 
syntax including loops and exception handling -- the laws of underlying
abstractions guarantee that these constructs preserve intuition about code. At the same time, 
the mechanism is \emph{adaptable} and enables appropriate syntax depending on the abstraction.

Most languages, including Haskell, Scala, C\#, JavaScript and Python have multiple
syntactic extensions that improve computational expressivity: queries, 
iterators, comprehensions, asynchronous computations are just a few. However,
``syntactic budget'' for such extensions is limited. Haskell already uses three notations
for comprehensions, monads and arrows \cite{arrows}. C\# and Scala have multiple notations
for queries, comprehensions, asynchronicity and iterators. The more we get with one 
mechanism, the better. As we show, computation expressions give a lot for relatively 
low cost -- notably, without resorting to full-blown macros.

Some of the technical aspects of the feature have been described before\footnote{F\# 3.0 extends the 
mechanism further to accomodate extensible query syntax. To keep this paper focused, we leave analysis 
of these extensions to future work.} \cite{fsharp-spec}, but this paper is novel in that it relates the mechanism to 
a range of well-known abstract computations. We also present new typing rules based on those uses.

\vspace{-1em}
\subsubsection{Practical examples.} 
We demonstrate the breadth of computations that can be structured using F\# computation expressions.
The applications include asynchronous workflows and sequences \S\ref{sec:intro-async}, 
\S\ref{sec:intro-asyncseq}, list comprehensions and monadic parsers \S\ref{sec:intro-seq-parsers}
and formlets for web programming \S\ref{sec:intro-formlets}.

\vspace{-1em}
\subsubsection{Abstract computations.} We show that the above examples fit well-known types
of abstract computations, including additive monads and monad transformers, and we show that 
important syntactic equalities hold as a result \S\ref{sec:laws}.

\vspace{-1em}
\subsubsection{Syntax and typing.} We give typing rules that capture idiomatic uses of computation
expressions \S\ref{sec:semantics-typing}, extend the translation to support applicative functors 
\S\ref{sec:intro-formlets} and discuss the treatment of effects \S\ref{sec:semantics-delayed} that 
is needed in impure languages.

\vspace{1.0em}
\noindent
We believe that software artifacts in programming language research matter~\cite{artifacts}, so all 
code can be run at: \url{http://tryjoinads.org/computations}. 
The syntax for applicative functors is a reserch extension; other examples require F\# 2.0.

% ==================================================================================================

\section{Computation expressions by example}
\label{sec:intro}

Computation expressions are blocks of code that represent computations with a non-standard 
aspect such as laziness, asynchronicity, state or other. The code inside the block is re-interpreted 
using a \emph{computation builder}, which is a record of operations that define the semantics,
but also syntax available in the block.

Computation expressions mirror the standard F\# syntax (let binding, loops, exception handling),
but support additonal computational constructs. For example \ident{let!} represents computational 
(monadic) alternative of let binding.

We first introduce the syntax and mapping to the underlying operations informally, but both are made 
precise later \S\ref{sec:semantics}. Readers unfamiliar with F\# may find additional explanation
in previous publications \cite{fsharp-spec}. To show the breadth of applications, 
we look at five examples arising from different abstractions.

% --------------------------------------------------------------------------------------------------

\subsection{Monadic asynchronous workflows}
\label{sec:intro-async}

Asynchronous workflows \cite{fs-async} allow writing non-blocking I/O using a mechanism based on the 
\emph{continuation monad} (with error handling etc.) The following example compares F\# code
with an equivalent in C\# using a single-purpose feature:

\begin{equation*}
\begin{array}{l}
\kvd{let}~\ident{getLength}~url=\ident{async}~\{\\
\quad \kvd{let!}~html = \ident{fetchAsync}~url\\
\quad \kvd{do!}~\ident{Async}.\ident{Sleep}~1000\\
\quad \kvd{return}~html.\ident{Length}\\
\}
\end{array}\qquad
\begin{array}{l}
\kvd{async}~\ident{Task}\langle\ident{string}\rangle~\ident{GetLength}(\kvd{string}~url)~\{\\
\quad \kvd{var}~html = \kvd{await}~\ident{FetchAsync}(url);\\
\quad \kvd{await}~\ident{Task}.\ident{Delay}(1000);\\
\quad \kvd{return}~html.\ident{Length};\\
\}
\end{array}
\end{equation*}

Both functions return a computation that expects a \emph{continuation} and then downloads a given 
URL, waits one second and passes content length to the continuation. The C\# version uses the built-in
$\ident{await}$ keyword to represent non-blocking waiting. In F\#, the computation is enclosed
in the $\ident{async}~\{ \ldots \}$ block, where \ident{async} is an identifier that refers to
the computation builder. 

Depending on the operations provided by the builder, different pre-defined keywords are allowed
in the computation block. The \ident{let!} keyword represents (monadic) composition and requires 
the \emph{Bind} operation. This operation also enables the \ident{do!} keyword which is equivalent 
to using \ident{let!} on an unit-returning computation. Finally, the \ident{return} keyword is 
mapped to the \emph{Return} operation:
%
\begin{equation*}
\begin{array}{l}
\ident{async}.\ident{Bind}(\ident{fetchAsync}(url), \kvd{fun}~html\rightarrow\\
\qquad \ident{async}.\ident{Bind}(\ident{Async}.\ident{Sleep}~1000, \kvd{fun}~()\rightarrow\\
\qquad\qquad \ident{async}.\ident{Return}(html.\ident{Length})))
\end{array}
\end{equation*}
%
The two operations form a monad and have the standard types: $\emph{Return}$ has a type $\alpha \rightarrow A\alpha$ 
and the required type of $\emph{Bind}$ is $A\alpha \times (\alpha \rightarrow A\beta) \rightarrow A\beta$ 
(we write $\alpha, \beta$ for universally qualified type variables and $\tau$ as for concrete types)
\footnote{For the purpose of this paper, we write type application using a light notation $T\tau$. }.

\vspace{-1em}
\subsubsection{Sequencing and effects.} Effectful expressions in F\# return \ident{unit}.
Assuming $e_1$ returns \ident{unit}, we can sequence expression using $e_1; e_2$. We can also
write effectful \ident{if} condition without the \ident{else} clause (which implicitly returns
the unit value in the \ident{false} case). Both have an equivalent in the computation expression syntax:
%
\begin{equation*}
\begin{array}{l}
\ident{async}~\{~ \kvd{if}~delay~\ident{then}~\kvd{do!}~\ident{Async}.\ident{Sleep}(1000)\\
\hspace{3.4em}     \ident{printfn}~ \texttt{"Starting..."}\\
\hspace{3.4em}     \kvd{return!}~ \ident{asyncFetch}(url) ~\}
\end{array}
\end{equation*}
%
If $delay$ is true, the workflow waits one second before downloading page and returning it.
The translation uses additional operations --\emph{Zero} represents monadic unit value,
\emph{Combine} corresponds to the ``;'' operator and \emph{Delay} embeds an effectful expression
in a (delayed) computation. For monads, these can be defined in terms of \emph{Bind} and \emph{Return},
but this is not the case for all computations \S\ref{sec:intro-seq-parsers}.

We also use the \ident{return!} keyword, which returns the result of a computation and requires
an operation \emph{ReturnFrom} of type $A\alpha \rightarrow A\alpha$. This is typically
implemented as an identity function -- its main purpose is to enable the \ident{return!} keyword in 
the syntax, as this may not be alway desirable \S\ref{sec:intro-seq-parsers}.

\begin{equation*}
\begin{array}{l}
\ident{async}.\ident{Combine} \\
\quad~(~~ (~\,\kvd{if}~delay~\kvd{then}~
   \ident{async}.\ident{Bind}(\ident{Async}.\ident{Sleep}(1000), \kvd{fun}~() \rightarrow \ident{async}.\ident{Zero}())\\
\quad\quad ~~~\,\kvd{else}~\ident{async}.\ident{Zero}()~),~ \ident{async}.\ident{Delay}(\kvd{fun} () \rightarrow\\
\quad\quad\quad\quad \ident{printfn}~\texttt{"Starting..."}\\
\quad\quad\quad\quad \ident{async}.\ident{ReturnFrom}(\ident{asyncFetch}(url))) ))
\end{array}
\end{equation*}
%
\emph{Zero} has a type $\ident{unit} \rightarrow A\,\ident{unit}$ and is inserted
when a computation does not return a value, here in both branches of \ident{if}. 
A computation returning unit can be composed with another using \emph{Combine} which
has a type $A\,\ident{unit} \times A\alpha \rightarrow A\alpha$ and corresponds to
``;''. It runs the left-hand side before returning the result of the right-hand side.
Finally, \emph{Delay}, of type $(\ident{unit} \rightarrow A\tau) \rightarrow A\tau$, is
used to wrap any effectful computations (like printing) in the monad to avoid performing
the effects before the first part of sequential computation is run.

% --------------------------------------------------------------------------------------------------

\subsection{Additive parsers and list comprehensions}
\label{sec:intro-seq-parsers}

Parsers \cite{monad-parsing} or list comprehensions differ in that they may return multiple values.
Such computations can be structured using additive monads (\ident{MonadPlus} in 
Haskell). These abstractions can be used with F\# computation expressions too. Interestingly, they 
require different typing of \emph{Zero} and \emph{Combine}. 

\vspace{-1em}
\subsubsection{Monadic parsers.} For parsers, we use the same notation as previously.
The difference is that we can now use \ident{return} and \ident{return!} repeatedly. The 
following parsers recognize one or more and zero or more repetitions of a given predicate:
%
\begin{equation*}
\hspace{-2em} \begin{array}{l}
\kvd{let rec}~\ident{zeroOrMore}~p = \ident{parse}~\{\\
\quad \kvd{return!}~\ident{oneOrMore}~p\\
\quad \kvd{return}~[\,] ~\}
\end{array}
\qquad
\begin{array}{l}
\kvd{and}~\ident{oneOrMore}~p = \ident{parse}~\{\\
\quad \kvd{let!}~x=p\\
\quad \kvd{let!}~xs = \ident{zeroOrMore}~p\\
\quad \kvd{return}~x::xs ~\}
\end{array}
\end{equation*}
%
The \ident{oneOrMore} function uses just the monadic interface and so its translation uses 
\emph{Bind} and \emph{Return}. The \ident{zeroOrMore} function is more interesting -- it combines
a parser that returns one or more occurrences with a parser that always succeeds and returns an empty 
list. This is achieved using the \emph{Combine} operation:
%
\begin{equation*}
\begin{array}{l}
\kvd{let rec}~\ident{zeroOrMore}~p = \ident{parse}.\ident{Delay}(\kvd{fun}~()\rightarrow \\
\quad \ident{parse}.\ident{Combine}(~\ident{parse}.\ident{ReturnFrom}(\ident{oneOrMore}~p), \\
\hspace{7.8em} \ident{parse}.\ident{Delay}(\kvd{fun} () \rightarrow \ident{parse}.\ident{Return}(\,[\,]\,)~)))
\end{array}
\end{equation*}
%
Here, \emph{Combine} represents the monoidal operation on parsers (either left-biassed or
non-deterministic choice) and it has a type $P\alpha \times P\alpha \rightarrow P\alpha$. Accordingly,
the \emph{Zero} operations is the unit of the monoid. It has a type $\ident{unit}\rightarrow P\alpha$,
representing a parser that returns no $\alpha$ values (rather than returning a single $\unit$ value).

For effectful sequencing of monads, it only makes sense to use unit-returning values in the left-hand
side of \emph{Combine} and as the result of \emph{Zero}. However, if a computation supports the monoidal
interface, it can combine multiple returned values. This shows that the computation 
expression mechanism needs certain flexibility -- the translation is the same, but the typing differs.

\subsubsection{List comprehensions.} Although list comprehensions implement the same abstract type
as parsers, it is desirable to use different syntax if we want to make the syntactic sugar comparable to 
built-in features in other languages. The following shows F\# list comprehension and Python generator
side-by-side:

\begin{equation*}
\hspace{-2em}
\begin{array}{l}
\ident{seq}~\{~ \kvd{for}~n~\kvd{in}~list~\kvd{do}  \\
\hspace{3.5em}  \kvd{yield}~n          \\
\hspace{3.5em}  \kvd{yield}~n*10 ~\}
\end{array}
\qquad\qquad
\begin{array}{l}
\kvd{for}~n~\kvd{in}~list:  \\
\quad \kvd{yield}~n          \\
\quad \kvd{yield}~n*10
\end{array}
\end{equation*}
%
The computations iterate over a source list and produce two results for each input. 
Monad comprehensions \cite{monad-compre} allow us to write $[\;n*10\;|\;n\leftarrow list\;]$
to multiply all elements by 10, but they are not expressive enough to capture the duplication. 
Doing that requires rewriting the code using combinators.

Although the F\# syntax looks different to what we have seen so far, it is actually very
similar. The \ident{for} and \ident{yield} constructs are translated to \emph{For} and 
\emph{Yield} operations which have the same form as \emph{Bind} and \emph{Return}, but provide
backing for a different syntax. The translation looks as follows:
%
\begin{equation*}
\begin{array}{l}
\ident{seq}.\ident{For}(list, \kvd{fun}~() \rightarrow \\
\quad \ident{seq}.\ident{Combine}(\ident{seq}.\ident{Yield}(n), \ident{seq}.\ident{Delay}(\kvd{fun}~() \rightarrow 
   \ident{seq}.\ident{Yield}(n * 10)))~)
\end{array}
\end{equation*}
%
\emph{Combine} concatenates multiple results and has the standard monoidal type
$[\alpha]~\times~[\alpha] \rightarrow [\alpha]$. \emph{For} has the type of monadic 
bind $[\alpha] \rightarrow (\alpha \rightarrow [\beta]) \rightarrow [\beta]$ and \emph{Yield} has a
type of monadic unit $\alpha \rightarrow [\alpha]$. We could have provided the \emph{Bind} 
and \emph{Return} operations in the \ident{seq} builder instead, but this leads to a less intuitive
syntax that requires users to write \ident{let!} for iteration and \ident{return} for yielding.

As the Python comparison shows, the flexibility of computation expressions means that they
are often close to a built-in syntax. The author of a concrete computation (\ident{parse}, 
\ident{seq}, \ident{async}, \ldots) decides what syntax is appropriate. For additive monads,
the choice can be made based on the laws that hold \S\ref{sec:laws-monads}.

% --------------------------------------------------------------------------------------------------

\subsection{Layered asynchronous sequences}
\label{sec:intro-asyncseq}
 
It is often useful to combine non-standard aspects of multiple computations. 
This is captured by monad transformers \cite{monad-transformers}. Although F\# does not support
higher-kinded types, monad transformers still provide a useful conceptual framework. 

For example, \emph{asynchronous sequences} \cite{async-seq} combine non-blocking asynchronous 
execution with the ability to return multiple results -- a file download can then produce data 
in 1kB buffers as they become available. Using $\ident{Async}\,\tau$ as the base type, we can
follow the list monad transformer \cite{list-trans} and define the type as:
%
\begin{equation*}
\begin{array}{lcl}
\kvd{type}~\ident{AsyncSeqInner}\,\tau &~=~& \ident{AsyncNil} ~|~
    \ident{AsyncCons}~\kvd{of}~\tau \times \ident{Async}\,\tau \\
\kvd{type}~\ident{AsyncSeq}\,\tau &=& \ident{Async}\,(\ident{AsyncSeqInner}\,\tau)
\end{array}
\end{equation*}
%
When given a continuation, asynchronous sequence calls it with either \ident{AsyncNil}
(the end of the sequence) or with \ident{AsyncCons} that carries a value together with
the tail of the asynchronous sequence. The flexibility of computation expression 
makes it possible to provide an elegant syntax for writing such computations:

\begin{equation*}
\begin{array}{l}
\kvd{let rec}~\ident{urlPerSecond}~n = \ident{asyncSeq}~\{ \\
\quad \kvd{do!}~\ident{Async}.\ident{Sleep}~1000 \\
\quad \kvd{yield}~\ident{getUrl}~i \\
\quad \kvd{yield!}~\ident{iterate}~(i+1) ~\}
\end{array}
\qquad
\begin{array}{l}
\kvd{let}~\ident{pagePerSecond}~urls = \ident{asyncSeq}~\{ \\
\quad \kvd{for}~url~\kvd{in}~\ident{urlPerSecond}~0~\kvd{do}\\
\quad\quad \kvd{let!}~html = \ident{asyncFetch}~url \\
\quad\quad \kvd{yield}~url, html ~\}
\end{array}
\end{equation*}
%
The \ident{urlPerSecond} function creates an asynchronous sequence that produces one URL per
second. It uses bind (\ident{do!}) of the asynchronous workflow monad to wait one second
and then composition of asynchronous sequences, together with \ident{yield} to produce the 
next URL. The \ident{pagePerSecond} function uses \ident{for} to iterate over (bind on) an
asynchronous sequence and then \ident{let!} to wait for (bind on) an asynchronous workflow.
The \ident{for} loop is asynchronous and lazy -- it's body is run each time the caller asks 
for the next result.

Asynchronous sequences form a monad and so we could use the standard notation for monads with
just \ident{let!} and \ident{return}. We would then need explicit lifting function that turns
an asynchronous workflow into an asynchronous sequence that returns a single value. However,
F\# computation expressions allow us to do better. We can define both \ident{For} and 
\ident{Bind} with the following types:
%
\begin{equation*}
\begin{array}{rcll}
\ident{asyncSeq}.\ident{For} &~:~& \ident{AsyncSeq}\,\alpha &
   \rightarrow (\alpha \rightarrow \ident{AsyncSeq}\,\beta) \rightarrow \ident{AsyncSeq}\,\beta\\
\ident{asyncSeq}.\ident{Bind} &~:~& \ident{Async}\,\alpha   &
   \rightarrow (\alpha \rightarrow \ident{AsyncSeq}\,\beta) \rightarrow \ident{AsyncSeq}\,\beta\\
\end{array}
\end{equation*}
%
We omit the translation of the above example -- it is a straightforward variation on what we have 
seen so far. A more important point is that we can again benefit from the fact that operations
of the computation builder are not restricted to a specific type (such as \emph{Bind} to represent
binding on some monad $M$).

As previously, the choice of the syntax is left to the author of the computation. Here, asynchronous
sequences are an additive monad and so we use \ident{for}/\ident{yield}. Underlying
asynchronous workflows are just monads, so it makes sense to add \ident{let!} that automatically
lifts a workflow to an asynchronous sequence.

An important aspect of the fact that asynchronous sequences can be described using a monad
transformer is that certain laws hold. In \S\ref{sec:laws-transf} we show how these map to the 
computation expression syntax.

% --------------------------------------------------------------------------------------------------

\subsection{Applicative formlets}
\label{sec:intro-formlets}

\emph{Applicative functors} \cite{applicative,idioms-obliv} are weaker (and thus more common) 
abstraction than monads. The difference between applicative and monadic computations is that 
monadic computation can perform different effects depending on values obtained earlier during the 
computation. On the other hand, the effects of applicative computation are fully determined 
by its structure. 

In other words, it is not possible to choose which computation to run (using \ident{let!} or 
\ident{do!}) based on values obtained in previous \ident{let!} bindings. The following 
example demonstrates this using a web form abstraction called formlets \cite{formlets}:
%
\begin{equation*}
\begin{array}{l}
\ident{formlet}~\{~ \kvd{let!}~name = \ident{Formlet}.\ident{textBox} \\
\hspace{4.1em} \kvd{and}~gender = \ident{Formlet}.\ident{dropDown}~[\texttt{"Male"}; \texttt{"Female"}] \\
\hspace{4.1em} \kvd{return}~name + \texttt{" "} + gender ~\}
\end{array}
\end{equation*}

The computation describes two aspects -- the rendering and the processing of entered values.
The rendering phase uses the fixed structure to produce HTML with text-box and drop-down elements.
In the processing phase, the values of \emph{name} and \emph{gender} are available and are used to
calculate the result of the form.

The structure of the form needs to be known without having access to specific values. The syntax
uses parallel binding (\kvd{let!}\ldots\kvd{and}\ldots), which binds a fixed number of independent
computations. The rest of the computation cannot contain other (applicative) bindings.

There are two equivalent definitions of applicative functors. We need two~operations known
from the less common style. \emph{Merge} of type $F\alpha \times F\beta \rightarrow F(\alpha\times\beta)$
represents composition of the structure (without considering specific values) and \emph{Map} of 
type $F\alpha \times (\alpha \rightarrow \beta) \rightarrow F\beta$ transforms the (pure) value.
The computation expression from the previous example is translated as follows:
%
\vspace{-0.3em}
\begin{equation*}
\begin{array}{l}
\ident{formlet}.\ident{Map} \\
\quad (~\ident{formlet}.\ident{Merge}(\ident{Formlet}.\ident{textBox}, 
  \ident{Formlet}.\ident{dropDown}~[\texttt{"Male"}; \texttt{"Female"}]),  \\
\quad~~\kvd{fun}~(name, gender) \rightarrow name + \texttt{" "} + gender~)
\end{array}
\vspace{-0.3em}
\end{equation*}
%
The computations composed using parallel binding are combined using \emph{Merge}. In formlets, 
this determines the structure used for HTML rendering. The rest of the computation is turned into 
a pure function passed to \emph{Map}. Note that the translation allows uses beyond applicative 
functors. The \ident{let!}\ldots\ident{and}\ldots syntax can also be used with monads to write 
zip comprehensions \cite{monad-compre}.

Applicative functors were first introduced to support \emph{applicative} programming style where
monads are not needed. The \emph{idiom brackets} notation \cite{applicative} fits that purpose better. We
find that computation expressions provide a useful alternative for more complex code and fit better 
with the impure nature of F\#.

% ==================================================================================================

\section{Semantics of computation expressions}
\label{sec:semantics}

The F\# language specification \cite{fsharp-spec} documents computation expressions as a purely syntactic 
mechanism. They are desugared before type-checking, which is then performed on the translated code 
using standard F\# typing rules. Similarly to Haskell's rebindable syntax, but to a greater level, 
this provides flexibility that allows the users to invent previously unforseen abstractions.

In this paper, we relate computation expressions to standard abstract computation types. In this
section, we present new typing rules that capture such common uses and make the system more robust 
by supporting better error messages and disallowing uncommon (likely erroneous) uses.

% --------------------------------------------------------------------------------------------------

\subsection{Syntax}
\label{sec:semantics-syntax}

The full syntax of computation expressions is given in the language specification, but the following
lists all important constructs that we consider in this paper:
%
\begin{equation*}
\begin{array}{lclcl}
\expr  &=& \expr ~\{~ \cexpr ~\}                       &\quad&\textnormal{(computation expression)}\\
\binds &=& v=\expr                                          &&\textnormal{(single binding)}\\
       &|& v=\expr ~\kvd{and}~\binds                        &&\textnormal{(parallel binding)}\\
\end{array}
\end{equation*}

\begin{equation*}
\begin{array}{lclcl}
\cexpr &=& \kvd{let}~v=\expr~\kvd{in}~\cexpr                &&\textnormal{(binding value)} \\
       &|& \kvd{let!}~\binds~\kvd{in}~\cexpr                &&\textnormal{(binding computation)} \\
       &|& \kvd{for}~v~\kvd{in}~\expr~\kvd{do}~\cexpr       &&\textnormal{(for loop computation)} \\
       &|& \kvd{return}~\expr                               &&\textnormal{(return value)} \\
       &|& \kvd{return!}~\expr                              &&\textnormal{(return computation)} \\
       &|& \kvd{yield}~\expr                                &&\textnormal{(yield value)} \\
       &|& \kvd{yield!}~\expr                               &&\textnormal{(yield computation)} \\
       &|& \cexpr_1; \cexpr_2                               &&\textnormal{(compose computations)} \\       
       &|& \expr                                            &&\textnormal{(effectful expression)} \\
\end{array}
\end{equation*}
%
We omit \ident{do!} which can be easily expressed using \ident{let!} To accommodate the applicative 
syntax, \binds\; is used to express one or more parallel variable bindings. 

For space reasons, we also omit imperative \ident{while} and exception handling 
constructs, but both of these are an important part of computation expressions. They allow
taking existing code and wrapping it in a computation block to augment it with non-standard
computational aspect.

% --------------------------------------------------------------------------------------------------

\begin{figure}[t!]
\begin{equation*}\hspace{-2em}
\begin{array}{rl}
\multicolumn{2}{c}{
  \boxed{\Gamma \vdash \expr :\tau}\quad\textnormal{and}\quad
    \boxed{\Gamma \bvdash \binds : \mtyp\Sigma}
}\\[1.5em]
%
(\textnormal{run})~
\inference
  { \Gamma \vdash \expr : \sigma  & \Gamma \cvdash \cexpr : \mtyp{\tau} }
  { \Gamma \vdash \expr ~\{~ \cexpr ~\} : \ntyp \tau }
&\quad\begin{array}{l}
(\forall \alpha: \sigma.\plc{Run}~:~\dtyp{\alpha} \rightarrow \ntyp \alpha \\
\;\,\forall\alpha: \sigma.\plc{Delay} : (\unit \rightarrow \mtyp{\alpha}) \rightarrow \dtyp{\alpha} )
\end{array}
\\[1.5em]
(\textnormal{bind-one})~
\inference 
  { \Gamma \vdash \expr : \mtyp{\tau} }
  { \Gamma \bvdash v=\expr : \mtyp{(\tya{v}{\tau})} }
\\[1.5em]
(\textnormal{bind-par})~
\inference 
  { \Gamma \vdash \expr : \tau & \Gamma \bvdash \binds : \mtyp{\Sigma} }
  { \Gamma \bvdash v=\expr~\kvd{and}~\binds : \mtyp{(\Sigma, \tya{v}{\tau})}  }
&\quad\begin{array}{l}
(\forall\alpha,\beta: \sigma.\plc{Merge}~:\\
\quad \mtyp{\alpha} \rightarrow \mtyp{\beta} \rightarrow \mtyp{(\alpha \times \beta)} )  
\end{array}
\\[1.5em]
%
\multicolumn{2}{c}{
  \boxed{\Gamma \cvdash \cexpr : \mtyp\tau}
}\\[1.5em]
%
(\textnormal{let})~
\inference
  { \Gamma \vdash \expr : \tau_1 &
    \Gamma, \tya{v}{\tau_1} \cvdash \cexpr : \mtyp{\tau_2} }
  { \Gamma \cvdash \kvd{let}~v=\expr~\kvd{in}~\cexpr : \mtyp{\tau_2}  }
\\[1.5em]
(\textnormal{bind})~
\inference
  { \Gamma \bvdash \binds : \mtyp\Sigma &
    \Gamma, \Sigma \cvdash \cexpr : \ntyp{\tau} }
  { \Gamma \cvdash \kvd{let!}~\binds~\kvd{in}~\cexpr : \ntyp{\tau}  }
&\quad\begin{array}{l}
(\forall\alpha,\beta: \sigma.\plc{Bind}~:\\
\quad \mtyp{\alpha} \rightarrow (\alpha \rightarrow \ntyp{\beta}) \rightarrow \ntyp{\beta} )  
\end{array}
\\[1.5em]
(\textnormal{map})~
\inference
  { \Gamma \bvdash \binds : \mtyp\Sigma &
    \Gamma, \Sigma \vdash \expr : \tau }
  { \Gamma \cvdash \kvd{let!}~\binds~\kvd{in}~\kvd{return}~\expr : \ntyp{\tau}  }
&\quad\begin{array}{l}
(\forall\alpha,\beta: \sigma.\plc{Map}~:\\
\quad \mtyp{\alpha} \rightarrow (\alpha \rightarrow \beta) \rightarrow \ntyp{\beta} )  
\end{array}
\\[1.5em]
(\textnormal{for})~
\inference
  { \Gamma \vdash \expr : \mtyp{\tau_1} &
    \Gamma, \tya{v}{\tau_1} \cvdash \cexpr : \ntyp{\tau_2} }
  { \Gamma \cvdash \kvd{for}~v~\kvd{in}~\expr~\kvd{do}~\cexpr : \ntyp{\tau_2}  }
&\quad\begin{array}{l}
(\forall\alpha,\beta: \sigma.\plc{For}~:\\
\quad \mtyp{\alpha} \rightarrow (\alpha \rightarrow \ntyp{\beta}) \rightarrow \ntyp{\beta} )  
\end{array}
\\[1.5em]
(\textnormal{return-val})~
\inference
  { \Gamma \vdash \expr : \tau }
  { \Gamma \cvdash \kvd{return}~\expr : \mtyp{\tau}  }
&\quad(\forall\alpha: \sigma.\plc{Return}~:~\alpha \rightarrow \mtyp{\alpha})
\\[1.5em]
(\textnormal{return-comp})~
\inference
  { \Gamma \vdash \expr : \mtyp \tau }
  { \Gamma \cvdash \kvd{return!}~\expr : \ntyp{\tau}  }
&\quad(\forall\alpha: \sigma.\plc{ReturnFrom}~:~\mtyp{\alpha} \rightarrow \ntyp{\alpha})  
\\[1.5em]
(\textnormal{seq})~
\inference
  { \Gamma \cvdash \cexpr_1 : \mtyp{\tau_1}  &  \Gamma \cvdash \cexpr_2 : \ntyp{\tau_2}}
  { \Gamma \cvdash \cexpr_1; \cexpr_2 : \ltyp{\tau_1}  }
&\quad\begin{array}{l}
(\forall\alpha: \sigma.\plc{Delay} ~:~ (\unit \rightarrow \ntyp{\alpha}) \rightarrow \dtyp{\alpha} \\
\;\,\forall\alpha: \sigma.\plc{Combine} : \mtyp{\tau_1} \rightarrow \dtyp{\alpha} \rightarrow \ltyp{\alpha} )
\end{array}
\\[1.5em]
(\textnormal{zero})~
\inference
  { \Gamma \vdash \expr : \unit }
  { \Gamma \cvdash \expr : \mtyp{\tau}  }
&\quad(\sigma.\plc{Zero}~:~\unit \rightarrow \mtyp{\tau})
\end{array}
\end{equation*}
\vspace{-1em}
\caption{Typing rules for computation expressions}
\label{fig:typing}
\vspace{-1em}
\end{figure}

% --------------------------------------------------------------------------------------------------

\subsection{Typing}
\label{sec:semantics-typing}

The Figure~\ref{fig:typing} uses three judgments. Standard F\# 
expressions are typed using $\Gamma \vdash \expr : \tau$. Computation expressions always return
computation of type $\mtyp{\tau}$ and are typed using $\Gamma \cvdash \cexpr : \mtyp{\tau}$.
A helper judgement $\Gamma \bvdash \binds : \mtyp{\Sigma}$ checks bindings
of multiple computations and produces a variable context with newly bound variables, 
wrapped in the type $M$ of the bound computations.

The latter two are parameterized by the type of the computation expression builder (such as
\ident{seq} or \ident{async}). The operations supported by the builder determine which syntactic
constructs are enabled. Typing rules that require a certain operation have a side-condition
on the right, which specifies the requirement.

In most of the side-conditions, the functions are universally quantified over the type of values
(written as $\alpha, \beta$). This captures the fact that computation should not restrict the 
values that users can work with. However, this is not the case in the rules (\emph{seq}) and 
(\emph{zero}). Here, we can only require that a specific instantiation is available -- the reason 
is that these operations may be used in two different ways. As discussed in \S\ref{sec:intro-async}, 
for monads the result of \emph{Zero} and the first argument of \emph{Combine} are restricted to
$M\,\ident{unit}$. They can be universally quantified only if the computation is monoidal 
\S\ref{sec:intro-seq-parsers}.

Another notable aspect of the typing is that a single computation expression may use multiple
computation types (written $M, N, L$ and $D$). In \emph{Bind} and \emph{For}, the type of bound
argument is $M$, but the resulting computation is $N$ (we require that bind returns the same
type of computation as the one produced by the function). This corresponds to the typing used
by computations arising from monad transformers \S\ref{sec:intro-asyncseq}. Although combining
multiple computation types is not as frequent, computations often have a delayed version which
we write as $D$. This is an important consideration for impure langauges such as F\# 
\S\ref{sec:semantics-delayed}.

Finally, we omitted typing for \ident{yield} and \ident{yield!} because it is similar to the 
typing of \ident{return} and \ident{return!} (using \emph{Yield} and \emph{YieldFrom} operations,
respectively).

% --------------------------------------------------------------------------------------------------

\begin{figure}[t]
\begin{equation*}
\hspace{-1em}\begin{array}{rcl}
\expr ~\{~ \cexpr ~\} &~=~& \kvd{let}~m = \expr ~\kvd{in}~m.\ident{Run}(m.\ident{Delay}(\kvd{fun}~() \rightarrow \tsl{\cexpr}))
\\[0.5em]
\tsl{\kvd{let}~v=\expr~\kvd{in}~\cexpr}          &=& \kvd{let}~v=\expr~\kvd{in}~\tsl{\cexpr} \\[0.08em]
\tsl{\kvd{let!}~\binds~\kvd{in}~\cexpr}          &=& m.\ident{Bind}(\tsb{\binds},\kvd{fun}~\tsv{\binds}\rightarrow \tsl{\cexpr}) \\[0.08em]
\tsl{\kvd{let!}~\binds~\kvd{in}~\kvd{return}\expr}          &=&
   m.\ident{Map}(\tsb{\binds},\kvd{fun}~\tsv{\binds}\rightarrow \expr) \\[0.08em]
\tsl{\kvd{for}~v~\kvd{in}~\expr~\kvd{do}~\cexpr} &=& m.\ident{For}(\expr,\kvd{fun}~()\rightarrow \tsl{\cexpr}) \\[0.08em]
%
\tsl{\kvd{return}~\expr}   &=& m.\ident{Return}(\expr) \\[0.08em]
\tsl{\kvd{return!}~\expr}  &=& m.\ident{ReturnFrom}(\expr) \\[0.08em]
\tsl{\cexpr_1; \cexpr_2}   &=& m.\ident{Combine}(\tsl{\cexpr_1}, \;m.\ident{Delay}(\kvd{fun}~() \rightarrow \tsl{\cexpr_2} )) \\[0.08em]
\tsl{\expr}                &=& \expr; ~m.\ident{Zero}()
\\[0.5em]
\tsb{v=\expr} &=& \expr \\[0.08em]
\tsb{v=\expr ~\kvd{and}~\binds} &=& m.\ident{Merge}(\expr, \tsl{\binds})
\\[0.5em]
\tsv{v=\expr} &=& v \\[0.08em]
\tsv{v=\expr ~\kvd{and}~\binds} &=& v, \tsv{\binds} 
\end{array}
\end{equation*}
\vspace{-1.2em}
\caption{Translation rules for computation expressions}
\label{fig:translation}
\vspace{-1em}
\end{figure}

% --------------------------------------------------------------------------------------------------

\subsection{Translation}
\label{sec:semantics-transl}

The translation is defined as a relation $\tsl{-}$ that is parameterized by a variable $m$ which
refers to the current instance of a computation builder. This parameter is used 
to invoke members of the builder, such as $m.\ident{Return}(\ldots)$. Multiple 
variable bindings are translated using $\tsb{\binds}$ and we define a helper mapping 
$\tsv{\binds}$ that turns bindings into a simple pattern that can be used to decompose a tuple
constructed by merging computations using the \emph{Merge} operation.

It is easy to check that our typing guarantees that a well-typed computation expression
is always be translated to a well-typed F\# expression. The side-conditions ensure that all
operations are available and have an appropriate type.

Careful readers have already noticed that our definition of $\tsl{-}$ is ambiguous. The 
\ident{let!} binding followed by \ident{return} can be translated in two different ways. 
We intentionally do not specify the behaviour in this paper -- the laws in \S\ref{sec:laws-monads} 
require the two translations to be equivalent. For monads, this equivalence is easy to see 
by considering the definition of \emph{Map} in terms of \emph{Bind} and \emph{Return}.

In earlier discussion, we omitted the \emph{Run} and \emph{Delay} members in the translation of
$\expr~\{\,\cexpr\,\}$. The next section discusses these two in more details.

% --------------------------------------------------------------------------------------------------

\subsection{Delayed computations}
\label{sec:semantics-delayed}

We already mentioned that side-effects are an important consideration when adding sequencing to
monadic comptuations \S\ref{sec:intro-async}. In effectful languages, we need to distinguish 
between two types of monads. We use the term \emph{monadic computation} for monads that represent 
a delayed computation such as asynchronous workflows or lazy list comprehensions; the term 
\emph{monadic containers} will be used for monads that represent a wrapped non-delayed value
(such as the option type, non-lazy list or the identity monad).

\vspace{-1em}
\subsubsection{Monadic computations.} The defining feature of \emph{monadic computations}
is that they permit a \emph{Delay} operation of type $(\unit \rightarrow M\alpha) \rightarrow M\alpha$
that does not perform the effects associated with the function argument.
For example, in asynchronous workflows, the operation builds 
a computation that waits for a continuation -- and so the effects are only run when the continuation
is provided.

Before going further, we revist the translation of asynchronous workflows using the full set of
rules to show how \emph{Run} and \emph{Delay} are used. Consider the the following simple computation
with a corresponding translation:
%
\begin{equation*}
\begin{array}{l}
\kvd{let}~\ident{answer} = \ident{async}~\{\\
\quad \ident{printfn}~ \texttt{"Welcome..."}\\
\quad \kvd{return}~ 42 ~\}
\end{array}
\qquad
\begin{array}{l}
\kvd{let}~\ident{answer} = \ident{async}.\ident{Run}(\ident{async}.\ident{Delay}(\kvd{fun}~()\rightarrow\\
\quad \ident{printfn}~ \texttt{"Welcome..."}\\
\quad \ident{async}.\ident{Return}(42)~))
\end{array}
\end{equation*}

For monadic computations such as asynchronous workflows, we do not expect that the defining
\ident{answer} will print ``Welcome''. This is achieved by the wrapping specified in the 
translation rule for the $\expr~\{\,\cexpr\,\}$ expression. 

In this case, the result of \emph{Delay} is a computation $A\,\ident{int}$ that encapsulates the
delayed effect. For monadic computations, \emph{Run} is a simple identity -- contrary
to what the name suggests, it does not run the computation (although that might be an interesting
use beyond standard abstract computations). The need for \emph{Run} becomes obvious when we look
at monadic containers.

\vspace{-1em}
\subsubsection{Effects and monadic containers.} For monadic containers, it is impossible to define
a \emph{Delay} operation that does not perform the effects and has a type $(\unit \rightarrow M\alpha) \rightarrow M\alpha$,
because the resulting type has no way of capturing unevaluated code. However, the (\emph{seq}) typing 
rule in Figure~\ref{fig:typing} permits an alternative typing. Consider the following example using 
the Maybe (option) monad:
%
\begin{equation*}
\begin{array}{l}
\ident{maybe}~\{~\kvd{if}~b=0~\kvd{then}~\kvd{return!}~\ident{None}\\
\hspace{3.85em}  \ident{printfn}~\texttt{"Calculating..."}\\
\hspace{3.85em}  \kvd{return}~a\,/\,b~\}\\
\end{array}
\end{equation*}
%
Using the same translation rules, \emph{Run}, \emph{Delay} and \emph{Delay} are inserted as follows:
%
\begin{equation*}
\begin{array}{l}
\ident{maybe}.\ident{Run}(\ident{maybe}.\ident{Delay}(\kvd{fun}~()\rightarrow \ident{maybe}.\ident{Combine}\\
\quad\quad (~(\kvd{if}~b=0~\kvd{then}~\ident{maybe}.\ident{ReturnFrom}(\ident{None})~
    \kvd{else}~\ident{maybe}.\ident{Zero}()), \\
\hspace{2.9em} \ident{maybe}.\ident{Delay}(\kvd{fun}~()\rightarrow \ident{printfn}~\texttt{"Calculating..."}\\
\hspace{12.8em}     \ident{maybe}.\ident{Return}(a\,/\,b))~)~))
\end{array}
\end{equation*}
%
The key idea is that we can use two different types -- $M\alpha$ for varlues representing
(evaluated) monadic containers and $\unit \rightarrow M\alpha$ for delayed computations. The 
operations then have the following types:
%
\begin{equation*}
\begin{array}{lcl}
\emph{Delay}   &~:~& (\unit \rightarrow M\alpha) \rightarrow (\unit \rightarrow M\alpha)\\
\emph{Run}     &~:~& (\unit \rightarrow M\alpha) \rightarrow M\alpha\\
\emph{Combine} &~:~& M\,\unit \rightarrow (\unit \rightarrow M\alpha) \rightarrow M\alpha
\end{array}
\end{equation*}
%
Here, the \emph{Delay} operation becomes just an identity that returns the function created by the
translation. In the translation, the result of \emph{Delay} can be passed either to \emph{Run}
or as the second argument of \emph{Delay}, so these need to be changed accordingly. The \emph{Run}
function now becomes important as it turns the delayed function into a value of the expected
type $M\alpha$ (by applying it).

\vspace{-1em}
\subsubsection{Unified treatment of effects.} In the typing rules \S\ref{sec:semantics-typing}, 
we did not explicitly list the two options, because they can be generalized. We require that the
result of \emph{Delay} is some (possibly different) abstract type $D\alpha$ representing delayed
computations. For monadic computations, the type is just $M\alpha$ and for monadic containers,
it is $\unit \rightarrow M\alpha$. Our typing is even more flexible, as it allows usage of 
multiple different computation types -- but treatment of effects is one example where this
additional flexibility is necessary.

Finally, it should be noted that we use a slight simplification. The actual F\# implementation 
does not strictly require \emph{Run} and \emph{Delay} in the translation of $\expr~\{\,\cexpr\,\}$. 
They are only used if they are present. 


% ==================================================================================================

\section{Computation expression laws}
\label{sec:laws}

Although computation expressions are not tied to any specific abstract computation type, we 
showed that they are usually used with well-known abstractions.
This means three good things. First, we get better understanding of what computations can be encoded
(and how). Second, we can add a more precise typing \S\ref{sec:semantics-typing}. Third, we know
that certain syntactic transformations (refactorings) preserve the meaning of computation. This
section looks at the last point. 

To keep the presentation in this section focused, we assume that there are no side-effects 
and we ignore \emph{Run} and \emph{Delay}.

% --------------------------------------------------------------------------------------------------

\subsection{Monoid and semigroup laws}
\label{sec:laws-monoids}

We start from the simplest structures. A semigroup $(S, \circ)$ consists of a set
$S$ and a binary operation $\circ$ such that $a \circ (b \circ c) = (a \circ b) \circ c$.
A computation expression corresponding to a semigroup defines only \emph{Combine} (of type
$M\alpha \times M\alpha \rightarrow M\alpha$). To allow appropriate syntax, we also add
\emph{YieldFrom} which is just the identity function. The associativity implies the following
syntactic equivalence:
%
\begin{equation*}
\ident{m}~\{~\cexpr_1; \,\cexpr_2; \,\cexpr_3 ~\} ~\equiv~
  \ident{m}~\{~\kvd{yield!}~\ident{m}~\{ \cexpr_1; \,\cexpr_2 ~\}; \,\cexpr_3 ~\}
\end{equation*}
%
A monoid $(S, \circ, \epsilon)$ is a semigroup $(S, \circ)$ with an identity element $\epsilon$ meaning 
that for all values $a\in S$ it holds that $\epsilon \circ a = a = a \circ \epsilon$. The identity
element can be added to computation builder as the \emph{Zero} member. This operation is used when
a computation uses conditional without \ident{else} branch. Thus we get:
%
\begin{equation*}
\begin{array}{l}
\ident{m}~\{~  \kvd{if}~\ident{false}~\kvd{then}~ \cexpr_1\\
\hspace{1.9em} \cexpr_2 ~\}
\end{array} 
~\equiv~
\ident{m}~\{~ \cexpr_2 ~\}\\
~\equiv~
\begin{array}{l}
\ident{m}~\{~  \cexpr_2 \\
\hspace{2.0em} \kvd{if}~\ident{false}~\kvd{then}~ \cexpr_1 ~\}
\end{array}
\end{equation*}
%
Although these are simple laws, they can be used to reason about list comprehensions. The associativity
means that we can move a part of computation expression (that uses \ident{yield!} repeatedly) into a
separate computation. To use the identity law, consider a recursive function that generates numbers
up to 100:
%
\begin{equation*}
\begin{array}{l}
\kvd{let}~\kvd{rec}~\ident{range}~n =  \ident{seq}~\{~ \kvd{yield}~n\\
\hspace{10.3em} \kvd{if}~n < 100~\kvd{then}~\kvd{yield!}~\ident{range}~(n + 1)~\}
\end{array}
\end{equation*}
%
The law guarantees that for $n=100$, the body is equivalent to $\ident{m}~\{~\ident{yield}~100~\}$.
This is an expected property of the \ident{if} construct -- the law guarantees that it holds even 
for \ident{if} that is reinterpreted by some (monoidal) computation expression.

% --------------------------------------------------------------------------------------------------

\subsection{Monad and additive monad laws}
\label{sec:laws-monads}

Monad laws are well-understood and the corresponding equivalent computation expressions do
not significantly differ from the laws about Haskell's do notation:
%
\begin{equation*}
\begin{array}{rl}
\ident{m}~\{~ \kvd{let!}~y=\ident{m}~\{~\kvd{return}~x~\}~\kvd{in}~\cexpr~ \}
&~\equiv~ \ident{m}~\{~ \kvd{let}~y=x~\kvd{in}~\cexpr~ \}
\\
\ident{m}~\{~ \kvd{let!}~x=c~\kvd{in}~\kvd{return}~x \}
&~\equiv~ \ident{m}~\{~ \kvd{return!}~c \}
\end{array}
\end{equation*}
\begin{equation*}
\begin{array}{rcl}
      &\ident{m}~\{~ \kvd{let!}~x=\ident{m}~\{~\kvd{let!}~y=c~\kvd{in}~\cexpr_1~\}~\kvd{in}~\cexpr_2~\}&\equiv\\
\equiv&\ident{m}~\{~ \kvd{let!}~y=c~\kvd{in}~\kvd{let!}~x=\ident{m}~\{~\cexpr_1~\}~\kvd{in}~\cexpr_2~\}\\
\end{array}
\end{equation*}

\vspace{-1em}
\subsubsection{Resolving ambiguity.} When discussing the translation rules 
\S\ref{sec:semantics-transl}, we noted that the rules are ambiguous when both \emph{Map} and
\emph{Bind} operations are present. The following can be translated both monadically and 
applicatively:
%
\begin{equation*}
\ident{m}~\{~\kvd{let!}~x = c~\kvd{in}~\kvd{return}~\expr~\}\\
\end{equation*}
%
The two translations are shown below. Assuming that our computation is a monad,
this is a well-known definition of \emph{Map} in terms of \emph{Bind} and \emph{Return}:
%
\begin{equation*}
\ident{m}.\ident{Map}(x, \kvd{fun}~x \rightarrow \expr) ~\equiv~
  \ident{m}.\ident{Bind}(x, \kvd{fun}~x \rightarrow \ident{m}.\ident{Return}(\expr))\\
\end{equation*}
%
More generally, if a computation builder defines both \emph{Map} and \emph{Bind} (even if they are
not based on a monad), we require this equation to guarantee that the two possible translations 
produce equivalent computations.

\subsubsection{Additive monads.} Additive monads are computations that combine monad with 
the monoidal structure. As shown earlier \S\ref{sec:intro-seq-parsers}, these can be embedded
using \ident{let!}/\ident{return} or using \ident{for}/\ident{yield}. The choice can be made
based on the laws that hold.

The laws required for additive monads is not fully resolved \cite{monadplus}. A frequently advocated
law is \emph{left distributivity} -- binding on the result of a monoidal operation is equivalent 
to binding on two computations and then combining the results:
%
\begin{equation*}
\ident{m}.\ident{For}(\ident{m}.\ident{Combine}(a, b), f)
~\equiv~
\ident{m}.\ident{Combine}(\ident{m}.\ident{For}(a, f), \ident{m}.\ident{For}(b, f))
\end{equation*}
%
We intentionally use the \emph{For} operation (corresponding to the \ident{for} keyword), because
this leads to the following intuitive syntactic equality:
%
\begin{equation*}
\begin{array}{l}
\ident{m}~\{~  \kvd{for}~x~\kvd{in}~\ident{m}~\{~\cexpr_1;\;\cexpr_2~\}~\kvd{do}\\
\hspace{2.0em}\quad \cexpr~\}
\end{array}~
\begin{array}{c}\equiv\\~\end{array}~
\begin{array}{l}
\ident{m}~\{~ \kvd{for}~x~\kvd{in}~\ident{m}~\{~\cexpr_1~\}~\kvd{do}~\cexpr \\
\hspace{2em}  \kvd{for}~x~\kvd{in}~\ident{m}~\{~\cexpr_2~\}~\kvd{do}~\cexpr~\}
\end{array}
\end{equation*}
%
If we read the code as an imperative looping construct (without the computational reinterpretation),
then this is, indeed, a valid law about \ident{for} loops.

Another law that is sometimes required about additive monads is \emph{left catch}. It states that
combining a computation that immediately returns a value with any other computation results in 
a computation that just returns the value:
%
\begin{equation*}
\hspace{-3em}
\ident{m}.\ident{Combine}(\ident{m}.\ident{Return}(v), a)
~~\equiv~~
\ident{m}.\ident{Return}(v)
\end{equation*}
%
This time, we intentionally used the \emph{Return} member instead of \emph{Yield}, because the law
corresponds to the following syntactic equivalence:
%
\begin{equation*}
\hspace{-3em}
\begin{array}{l}
\ident{m}~\{~  \kvd{return}~v; ~ \cexpr~\}
\end{array}~~\equiv~~
\begin{array}{l}
\ident{m}~\{~  \kvd{return}~v~\}\\
\end{array}
\end{equation*}
%
The fact that \emph{left catch} corresponds to an intuitive syntactic equality about 
\ident{let!}/\ident{return} while \emph{left distributivity} corresponds to an intuitive syntactic equality
about \ident{for}/\ident{yield} determines the appropriate syntax. The former can be used
for list comprehensions (and other collections), while the latter is suitable e.g. for the 
option monad or the software transactional memory monad \cite{stm}.

\subsection{Monad transformers}
\label{sec:laws-transf}

There are multiple ways of composing or layering monads \cite{monad-transformers,monad-compose,monads-layered}. 
Monad transformers are perhaps the most widely known technique. A monad transformer is a type 
constructor $T\,m$ together with a \emph{Lift} operation. For some monad $M$ the operation has 
a type $M\,\alpha \rightarrow T\,M\,\alpha$ and it turns a computation in the underlying monad 
into a computation in the composed monad.

The result of monad transformer is also a monad. This means that we can use the usual syntactic 
sugar for monads, such as the do notation in Haskell. However, a more specific notation can
use the additional \emph{Lift} operation.

We looked at computation expression for a composed monad when discussing
asynchronous sequences \S\ref{sec:intro-asyncseq}. An asynchronous sequence $\ident{AsyncSeq}\,\alpha$ 
is a computation obtained by applying the list monad transformer \cite{list-trans} to the 
monad $\ident{Async}\,\alpha$. Asynchronous sequences are \emph{additive monads} satisfying 
the left distributivity law, so we choose the \ident{for}/\ident{yield} syntax for working with 
the composed computation. We also provided an additional \emph{Bind} operation to support 
awaiting a single asynchronous workflow using \ident{let!} This operation is
defined in terms of \emph{Lift} of the monad transformer and \emph{For} (monadic bind) of the
composed computation:
%
\begin{equation*}
\ident{asyncSeq}.\ident{Bind}(a, f) = \ident{asyncSeq}.\ident{For}(\ident{asyncSeq}.\ident{Lift}(a), f)
\end{equation*}
%
There are two laws that hold about monad transforers. To simplify the presentation, we use asynchronous 
workflows and sequences rather than showing the generalised version. The first law states 
that composing \emph{Return} of asynchronous workflows with \emph{Lift} should be equivalent to the 
\emph{Yield} of asynchronous sequences. The other states that \emph{Lift} distributes over monadic
bind. 

Our syntax always combines \emph{Lift} with \emph{For}, so the following syntactic equivalences
also require right identity for monads and function extensionality:
%
\begin{equation*}
\begin{array}{c}
\ident{asyncSeq}~\{~ \kvd{let!}~x = \ident{async}~\{~\kvd{return}~v~\}~\kvd{in}~\kvd{return}~x ~\} ~\equiv~
  \ident{asyncSeq}~\{~ \kvd{return}~v ~\}
\\[0.7em]
\ident{asyncSeq}~\{~ \kvd{let!}~x = \ident{async}~\{~\kvd{let!}~y = c~\kvd{in}~\cexpr_1~\}~\kvd{in}~\cexpr_2 ~\} ~\equiv~\\
~\equiv~ \ident{asyncSeq}~\{~ \kvd{let!}~y = c~\kvd{in}~\kvd{let!}~x = \ident{async}~\{~\cexpr_1~\}~\kvd{in}~\cexpr_2 ~\}
\end{array}
\end{equation*}
%
The first equation returns $v$ without any asynchronous waiting in both cases
(although, in presence of side-effects, this is made more complicated by cancellation). 
The second equation is more subtle. The left-hand side awaits a single asynchronous workflow 
that first awaits $c$ and then does more work. The right-hand side awaits $c$ lifted to an 
asynchronous sequence and then awaits the rest.

% --------------------------------------------------------------------------------------------------

\subsection{Applicative computations}
\label{sec:laws-appl}

The last type of computations that we discussed \S\ref{sec:intro-formlets} is \emph{applicative functor}.
We use the less common definition called \ident{Monoidal} \cite{applicative}. It consists of \emph{Map} 
and \emph{Merge}, together with a unit computation. We use the unit to define \emph{Zero} -- it will be 
used in computations that contain some unit-returning expression, such as $()$.

The identity law guarantees that merging with a unit and then projecting the non-unit value
produces an equivalent computation:

\begin{equation*}
\begin{array}{l}
\ident{f}~\{~  \kvd{let!}~x = \ident{f}~\{~()~\}\\
\hspace{1.4em} \kvd{and}~y = c~\kvd{in}~\kvd{return}~y~\}
\}
\end{array}
~\equiv~ c ~\equiv~
\begin{array}{l}
\ident{f}~\{~  \kvd{let!}~x = c\\
\hspace{1.4em} \kvd{and}~y = \ident{f}~\{~()~\}~\kvd{in}~\kvd{return}~x~\}
\}
\end{array}
\end{equation*}
%
The naturality law specifies that \emph{Merge} distributes over \emph{Map}, which translates to
the following code (assuming $x_1$ not free in $\expr_2$ and $x_2$ not free in $\expr_1$):
%
\begin{equation*}
\begin{array}{c}
\begin{array}{l}
\ident{f}~\{~ \kvd{let!}~y_1 = \ident{f}~\{~\kvd{let!}~x_1 = c_1~\kvd{in}~\kvd{return}~\expr_1 ~\} \\
\hspace{1.40em}   \kvd{and}~y_2 = \ident{f}~\{~\kvd{let!}~x_2 = c_2~\kvd{in}~\kvd{return}~\expr_2 ~\}~\kvd{in}~ \expr~\}
\end{array} ~\equiv~
\\[1.2em]
~\equiv~
\begin{array}{l}
\ident{f}~\{~ \kvd{let!}~x_1 = c_1~\kvd{and}~x_2 = c_2~\kvd{in}~ 
              \kvd{let}~y_1, y_2 = \expr_1, \expr_2~\kvd{in}~ \expr~\}
\end{array}
\end{array}
\end{equation*}
%
As with the earlier syntactic rules, we can leave out the non-standard aspect
of the computations, read them as ordinary functional code and get correct and 
expected laws. This means that the laws, again, guarantee that intuition about the
syntax used by computation expressions will be correct.

Finally, the \emph{Merge} operation is also required to be associative -- this does not have
any corresponding syntax, but it means that the user does not need to know implementation
details of the compiler -- it does not matter whether the parsing of $\binds$ in 
\ident{let!}\ldots\ident{and}\ldots\ is left-associative or right-associative.

% ==================================================================================================

\section{Related work}

Haskell and its extensions support monad comprehensions \cite{monad-compose} and ``do'' notation
for monads, idiom brackets \cite{applicative} for applicatives and arrows \cite{arrows}. 
These are similar to computation expressions in that they are not tied to concrete computations. 
However, they differ syntactically -- they add multiple new notations, while computation expressions 
add a uniform notation resembling standard language structures. Adding arrows to
computation expressions is an open question.

Python and C\# generators, LINQ \cite{linq} in C\# and ``for'' comprehensions 
in Scala are just a few examples of syntax for concrete computations. Although they can all be 
used with other computations, this is not generally considered idiomatic use. Similarly to F\#, 
the Scala async library \cite{scala-async} supports loops and exception handling. However, it is
implemented through full macro system.

Aside from monads, other ways of encoding effectful computations include effect handlers
\cite{effect-handlers} and continuations \cite{monads-layered}. Providing syntactic support for
these encodings may provide an interesting alternative to our encoding using computation builder.
Interestingly, our \emph{Run} operation seems related to \emph{reset} of delimited continuations
\cite{delimcont} and  our \emph{Delay} is similar to the \emph{reify} operation of Filinsky 
\cite{monads-inaction}.



% ==================================================================================================

\section{Conclusions}
\label{sec:conclusions}

In this paper, we related F\# \emph{computation expressions} to well-known abstract computation 
types. Computation expressions provide a unified way for writing a wide range of computations 
including monoids, monads, applicative formlets and monads composed using monad transformers.

Computation expressions follow a different approach than e.g. Haskell ``do'' notation. They
integrate a wide range of abstractions and flexibly reuse existing syntax (including loops and 
exception handling). The library developer can choose the appropriate syntax and use laws of
abstract computations to guarantee that the computation preserves intuition about the syntax.

Such reusable syntactic extensions are becoming increasingly important. We cannot keep adding
new features to support comprehensions, asynchronicity, queries and more as the ``syntactic budget''
is rapidly running out.

% ==================================================================================================

%
% \subsubsection{Acknowledgements.} We are grateful to Dominic Orchard, Alan Mycroft, Sam Lindley,
% anonymous reviewers of previous draft and the audience of TFP 2012. 
%

\vspace{-1em}
\bibliographystyle{abbrv}
\bibliography{computation-zoo}


\end{document} 

