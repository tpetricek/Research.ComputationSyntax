\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{semantic}
\usepackage{verbatim}
\usepackage{stmaryrd}
\usepackage{color}
\usepackage{textcomp}
\usepackage{hyperref}
\usepackage{breakurl}

\begin{document}

\title{The F\# Computation Expressions Zoo}
\author{Tomas Petricek\inst{1} \and Don Syme\inst{2}}
\institute{University of Cambridge, UK\and Microsoft Research Cambridge, UK \\
\textsf{tp322@cam.ac.uk}, \textsf{dsyme@microsoft.com}}


\maketitle

% ==================================================================================================

\newcommand{\sep}[0]{\; | \;}
\newcommand{\kvd}[1]{\textnormal{\bfseries\sffamily #1}}
\newcommand{\plc}[1]{\textnormal{\emph{#1}}}
\newcommand{\ident}[1]{\textnormal{\sffamily #1}}

\newcommand{\cexpr}{\plc{cexpr}}
\newcommand{\expr}{\plc{expr}}
\newcommand{\binds}{\plc{binds}}
\newcommand{\pat}{\plc{pat}}

% Computation types
\newcommand{\mtyp}[1]{M #1}
\newcommand{\ntyp}[1]{N #1}
\newcommand{\ltyp}[1]{L #1}
\newcommand{\dtyp}[1]{D #1}

% Typing rules etc.
\newcommand{\tya}[2]{#1\hspace{-0.15em}:\hspace{-0.15em}#2}
\newcommand{\cvdash}{\Vdash_\sigma}
\newcommand{\bvdash}{\vartriangleright_\sigma}
\newcommand{\unit}{\ident{unit}}

% Translation rules
\newcommand{\tsl}[1]{|[ \,#1\, |]_m}
\newcommand{\tsb}[1]{\langle\hspace{-0.25em}\langle #1 \rangle\hspace{-0.25em}\rangle_m}
\newcommand{\tsv}[1]{\langle #1 \rangle}

% ==================================================================================================

\begin{abstract}
Many computations can be structured using abstract types such as monoids, monad transformers or 
applicative functors. Functional programmers use those abstractions directly, but main-stream
languages often integrate concrete instances as language features -- e.g. generators in Python
or asynchronous computations in C\# 5.0. The question is, is there a sweet spot between a series of
convenient, hardwired language features, and an inconvenient but flexible set of libraries?

\quad F\# \emph{computation expressions} answer this question in the affirmative. Unlike 
the ``do'' notation in Haskell, computation expressions are not tied to a single kind of abstraction.
They support a wide range of computations, depending on what operations are available. They also 
provide greater syntactic flexibility leading to a more intuitive syntax, without resorting to 
full macro-based meta-programming. 

\quad We show that computation expressions can structure well-known computations including monoidal 
list comprehensions, monadic parsers, applicative formlets and asynchronous sequences based on the 
list monad transformer. We also present typing rules for computation expressions that are capable of 
capturing all these applications. 

\end{abstract}

% ==================================================================================================

\section{Introduction}
Computations with non-standard aspects like non-determinism, effects, asynchronicity or their
combinations can be captured using a variety of abstract computation types. In Haskell, we write 
such computations using a mix of combinators and syntactic extensions like monad comprehensions 
\cite{monad-compre} and ``do'' notation. Languages such as Python and C\# emphasize the syntax 
and provide single-purpose support e.g. for asynchrony \cite{cs-async} and list generators
\cite{csharp-iterators}. 

Using such abstractions can be made simpler and more intuitive 
if we employ a general syntactic machinery. F\# computation expressions provide
\emph{uniform} syntax that supports monoids, monads \cite{monads-fp}, monad transformers 
\cite{monad-transformers} and applicative functors \cite{applicative}. They reuse familiar 
syntax for including loops and exception handling -- the laws of underlying
abstractions guarantee that these constructs preserve intuition about code. At the same time, 
the mechanism is \emph{adaptable} and enables appropriate syntax depending on the abstraction.

Most languages, including Haskell, Scala, C\#, JavaScript and Python have multiple
syntactic extensions that improve computational expressivity: queries, 
iterators, comprehensions, asynchronous computations are just a few. However,
``syntactic budget'' for such extensions is limited. Haskell already uses three notations
for comprehensions, monads and arrows \cite{arrows}. C\# and Scala have multiple notations
for queries, comprehensions, asynchronicity and iterators. The more we get with one 
mechanism, the better. As we show, computation expressions give a lot for relatively 
low cost -- notably, without resorting to full-blown macros.

Some of the technical aspects of the feature have been described before\footnote{F\# 3.0 extends the 
mechanism further to accomodate extensible query syntax. To keep this paper focused, we leave analysis 
of these extensions to future work.} \cite{fsharp-spec}, but this paper is novel in that it relates the mechanism to 
a range of well-known abstract computations. We also present new typing rules based on those uses.

\vspace{-1em}
\subsubsection{Practical examples.} 
We demonstrate the breadth of computations that can be structured using F\# computation expressions.
The applications include asynchronous workflows and sequences \S\ref{sec:intro-async}, 
\S\ref{sec:intro-asyncseq}, list comprehensions and monadic parsers \S\ref{sec:intro-seq-parsers}
and formlets for web programming \S\ref{sec:intro-formlets}.

\vspace{-1em}
\subsubsection{Abstract computations.} We show that the above examples fit well-known types
of abstract computations, including additive monads and monad transformers, and we show what 
syntactic equalities hold as a result \S\ref{sec:laws}.

\vspace{-1em}
\subsubsection{Syntax and typing.} We give typing rules that capture idiomatic uses of computation
expressions \S\ref{sec:semantics-typing}, extend the translation to support applicative functors 
\S\ref{sec:intro-formlets} and discuss the treatment of effects \S\ref{sec:semantics-delayed} that 
is needed in impure languages.

\vspace{1.0em}
\noindent
We believe that software artifacts in programming language research matter~\cite{artifacts}, so all 
code can be run at: \url{http://tryjoinads.org/computations}. 
The syntax for applicative functors is a reserch extension; other examples require F\# 2.0.

% ==================================================================================================

\section{Computation expressions by example}
\label{sec:intro}

Computation expressions are blocks of code that represent computations with a non-standard 
aspect such as laziness, asynchronicity, state or other. The code inside the block is re-interpreted 
using a \emph{computation builder}, which is a record of operations that define the semantics,
but also syntax available in the block.

Computation expressions mirror the standard F\# syntax (let binding, loops, exception handling),
but support additonal computational constructs. For example \ident{let!} represents computational 
(monadic) alternative of let binding.

We first introduce the syntax and mapping to the underlying operations informally, but both are made 
precise later \S\ref{sec:semantics}. Readers unfamiliar with F\# may find additional explanation
in previous publications \cite{fsharp-expert,fsharp-spec}. To show the breadth of applications, 
we look at five examples arising from different abstractions.

% --------------------------------------------------------------------------------------------------

\subsection{Monadic asynchronous workflows}
\label{sec:intro-async}

Asynchronous workflows \cite{fs-async} allow writing non-blocking I/O using a mechanism based on the 
\emph{continuation monad} (with error handling etc.) The following example shows F\# version
with an equivalent C\# code using single-purpose feature:

\begin{equation*}
\begin{array}{l}
\kvd{let}~\ident{getLength}~url=\ident{async}~\{\\
\quad \kvd{let!}~html = \ident{fetchAsync}~url\\
\quad \kvd{do!}~\ident{Async}.\ident{Sleep}~1000\\
\quad \kvd{return}~html.\ident{Length}\\
\}
\end{array}\qquad
\begin{array}{l}
\kvd{async}~\ident{Task}\langle\ident{string}\rangle~\ident{GetLength}(\kvd{string}~url)~\{\\
\quad \kvd{var}~html = \kvd{await}~\ident{FetchAsync}(url);\\
\quad \kvd{await}~\ident{Task}.\ident{Delay}(1000);\\
\quad \kvd{return}~html.\ident{Length};\\
\}
\end{array}
\end{equation*}

Both functions return a computation that expects a \emph{continuation} and then downloads a given 
URL, waits one second and passes content length to the continuation. The C\# version uses the built-in
$\ident{await}$ keyword to represent non-blocking waiting. In F\#, the computation is enclosed
in the $\ident{async}~\{ \ldots \}$ block, where \ident{async} is an identifier that refers to
the computation builder. 

Depending on the operations provided by the builder, different pre-defined keywords are allowed
in the computation block. The previous snippet uses \ident{let!} which represents (monadic) 
composition and requires the \emph{Bind} operation. This operation also enables the \ident{do!} 
keyword which is equivalent to using \ident{let!} on an unit-returning computation. Finally, the 
\ident{return} keyword is mapped to the \emph{Return} operation, so the previous F\# snippet is 
translated as follows:
%
\begin{equation*}
\begin{array}{l}
\ident{async}.\ident{Bind}(\ident{fetchAsync}(url), \kvd{fun}~html\rightarrow\\
\qquad \ident{async}.\ident{Bind}(\ident{Async}.\ident{Sleep}~1000, \kvd{fun}~()\rightarrow\\
\qquad\qquad \ident{async}.\ident{Return}(html.\ident{Length})))
\end{array}
\end{equation*}
%
The two operations form a monad and have the standard types. Assuming $A\tau$ is a type of asynchronous 
computations, the $\emph{Return}$ has a type $\alpha \rightarrow A\alpha$ and the required type of
$\emph{Bind}$ is $A\alpha \rightarrow (\alpha \rightarrow A\beta) \rightarrow A\beta$ (as a convention,
we use $\alpha, \beta$ for universally qualified type variables and $\tau$ as for concrete types)
\footnote{For the purpose of this paper, we write type application using a light notation $T\tau$. }.

\vspace{-1em}
\subsubsection{Sequencing and effects.} Primitive effectful expressions in F\# return \ident{unit}.
Assuming $e_1$ returns \ident{unit}, we can sequence expression using $e_1; e_2$ and we can also
write effectful \ident{if} condition without the \ident{else} clause (which implicitly returns
the unit value in the \ident{false} case). Both of these constructs have their equivalent 
in the computation expression syntax:
%
\begin{equation*}
\begin{array}{l}
\ident{async}~\{~ \kvd{if}~delay~\ident{then}~\kvd{do!}~\ident{Async}.\ident{Sleep}(1000)\\
\hspace{3.4em}     \ident{printfn}~ \texttt{"Starting..."}\\
\hspace{3.4em}     \kvd{return!}~ \ident{asyncFetch}(url) ~\}
\end{array}
\end{equation*}
%
If $delay$ is true, the workflow waits one second before downloading page and returning it.
For monads, it is possible to translate the snippet above using just \emph{Bind} and \emph{Return}, 
but this approach does not work for other computations \S\ref{sec:intro-seq-parsers}. For this reason, 
F\# requires additional operations -- \emph{Zero} represents monadic unit value, \emph{Combine} 
corresponds to the ``;'' operator and \emph{Delay} takes an effectful computation and embeds the 
effects in a (delayed) computation. 

We also use the \ident{return!} keyword, which returns the result of a computation and requires
an operation \emph{ReturnFrom} of type $A\alpha \rightarrow A\alpha$. This is typically
implemented as an identity function -- its main purpose is to enable the \ident{return!} keyword in 
the syntax, as this may not be alway desirable \S\ref{sec:intro-seq-parsers}.
%
\begin{equation*}
\begin{array}{l}
\ident{async}.\ident{Combine}( \\
\quad\quad (~~\,\kvd{if}~delay~\kvd{then}~
   \ident{async}.\ident{Bind}(\ident{Async}.\ident{Sleep}(1000), \kvd{fun}~() \rightarrow \ident{async}.\ident{Zero}())\\
\quad\quad ~~~\,\kvd{else}~\ident{async}.\ident{Zero}()~),\\
\quad\quad \ident{async}.\ident{Delay}(\kvd{fun} () \rightarrow\\
\quad\quad\quad \ident{printfn}~\texttt{"Starting..."}\\
\quad\quad\quad \ident{async}.\ident{ReturnFrom}(\ident{asyncFetch}(url))) ))
\end{array}
\end{equation*}
%
The \emph{Zero} operation has a type $\ident{unit} \rightarrow A\,\ident{unit}$. It is inserted
when a computation does not return a value -- here, in both branches of the conditional. The
result of conditional is composed with the rest of the computation using \emph{Combine} which has
a type $A\,\ident{unit} \rightarrow A\alpha \rightarrow A\alpha$. The first argument is a unit-returning
computation, which mirrors the ``;'' operator -- the overall computation runs the left-hand side and
then returns the result of the right-hand side.

Finally, the \emph{Delay} operation (of type $(\ident{unit} \rightarrow A\tau) \rightarrow A\tau$ is
used to wrap any effectful computations (like printing) in the monadic computation to avoid evaluating
them before the first part of sequential computation is run.

% --------------------------------------------------------------------------------------------------

\subsection{Additive parsers and list comprehensions}
\label{sec:intro-seq-parsers}

An asynchronous workflow returns only \emph{one} value, but parsers or list comprehensions may return 
multiple values. Such computations can be structured using additive monads (\ident{MonadPlus} in 
Haskell). These abstractions can be used with F\# computation expressions, but they require different
typing of \emph{Zero} and \emph{Combine}. It may be also desirable to use different syntax.

\vspace{-1em}
\subsubsection{Monadic parsers.} For monadic parsers, we use a notation similar to the one used in
asynchronous workflows. The difference is that we can now use \ident{return} and \ident{return!} 
repeatedly. The following parsers recognize one or more and zero or more repetitions of a given
predicate, respectively:
%
\vspace{-0.5em}\begin{equation*}
\hspace{-2em} \begin{array}{l}
\kvd{let rec}~\ident{zeroOrMore}~p = \ident{parse}~\{\\
\quad \kvd{return!}~\ident{oneOrMore}~p\\
\quad \kvd{return}~[\,] ~\}
\end{array}
\qquad
\begin{array}{l}
\kvd{and}~\ident{oneOrMore}~p = \ident{parse}~\{\\
\quad \kvd{let!}~x=p\\
\quad \kvd{let!}~xs = \ident{zeroOrMore}~p\\
\quad \kvd{return}~x::xs ~\}
\end{array}
\vspace{-0.5em}
\end{equation*}
%
The \ident{oneOrMore} function uses just the monadic interface and so its translation uses 
\emph{Bind} and \emph{Return}. The \ident{zeroOrMore} function is more interesting -- it combines
a parser that returns one or more occurrences with a parser that always succeeds and returns an empty 
list. This is achieved using the \emph{Combine} operation:
%
\begin{equation*}
\begin{array}{l}
\kvd{let rec}~\ident{zeroOrMore}~p = \ident{parse}.\ident{Delay}(\kvd{fun}~()\rightarrow \\
\quad \ident{parse}.\ident{Combine}(~\ident{parse}.\ident{ReturnFrom}(\ident{oneOrMore}~p), \\
\hspace{7.8em} \ident{parse}.\ident{Delay}(\kvd{fun} () \rightarrow \ident{parse}.\ident{Return}(\,[\,]\,)~)))
\end{array}
\end{equation*}
%
The \emph{Combine} operation represents the monoidal operation on parsers (either left-biassed or
non-deterministic choice) and it has a type $P\alpha \rightarrow P\alpha \rightarrow P\alpha$. Accordingly,
the \emph{Zero} operations is the unit of the monoid. It represents a parser that always fails (returning
no values of type $\alpha$) and has a type $\ident{unit}\rightarrow P\alpha$.

For effectful sequencing of monads, it only makes sense to use unit-returning values in the left hand
side of \emph{Combine} and as the result of \emph{Zero}. However, if a computation supports the monoidal
interface, these operations can combine multiple returned values. This shows that the computation 
expression mechanism needs certain flexibility -- although the translation is the same in both cases,
the typing needs to depend on the user-defined types of the operations.

\vspace{-1em}
\subsubsection{List comprehensions.} Although list comprehensions implement the same abstract type
as parsers, we need to use different syntax if we want to make the syntactic sugar comparable to 
built-in features in other languages. The following shows F\# list comprehension and Python generator
side-by-side:

\begin{equation*}
\hspace{-2em}
\begin{array}{l}
\kvd{let}~\ident{duplicate}(list) = \ident{seq}~\{\\
\quad \kvd{for}~n~\kvd{in}~list~\kvd{do}  \\
\quad\quad \kvd{yield}~n          \\
\quad\quad \kvd{yield}~n*10 ~\}
\end{array}
\qquad\qquad
\begin{array}{l}
\kvd{def}~\ident{duplicate}(list):\\
\quad \kvd{for}~n~\kvd{in}~list:  \\
\quad\quad \kvd{yield}~n          \\
\quad\quad \kvd{yield}~n*10
\end{array}
\end{equation*}
%
The computations look very similar -- they iterate over a source list and produce two results
for each input. In contrast, Haskell monad comprehensions [19] allow us to write $[\;n*10\;|\;n\leftarrow list\;]$
to multiply all elements by 10, but they are not expressive enough to capture duplication. To do 
that, the code needs to use the monoidal operation (\ident{mplus}), but that cannot be done
inside comprehensions.

Although the F\# syntax looks different to what we have seen so far, it is actually very
similar. The \ident{for} and \ident{yield} constructs are translated to \emph{For} and 
\emph{Yield} operations which have the same form as \emph{Bind} and \emph{Return}, but provide
backing for a different syntax. The translation looks as follows:
%
\begin{equation*}
\begin{array}{l}
\ident{seq}.\ident{Delay}(\kvd{fun}~() \rightarrow \ident{seq}.\ident{For}(list, \kvd{fun}~() \rightarrow \\
\quad \ident{seq}.\ident{Combine}(\ident{seq}.\ident{Yield}(n), \ident{seq}.\ident{Delay}(\kvd{fun}~() \rightarrow 
   \ident{seq}.\ident{Yield}(n * 10)))~))
\end{array}
\end{equation*}
%
The \emph{Combine} operation concatenates multiple results and has the standard monoidal type
$[\alpha] \rightarrow [\alpha] \rightarrow [\alpha]$. The type of \emph{For} is that of monadic 
binding $[\alpha] \rightarrow (\alpha \rightarrow [\beta]) \rightarrow [\beta]$ and \emph{Yield} has a
type of monadic unit $\alpha \rightarrow [\alpha]$. We could have provided the \emph{Bind} 
and \emph{Return} operations in the \ident{seq} builder instead, but this leads to a less intuitive
syntax that requires users to write \ident{let!} for iteration and \ident{return} for yielding.

As the Python comparison shows, the flexibility of computation expressions means that they
are often close to built-in language features. The author of a concrete computation (\ident{parse}, 
\ident{seq}, \ident{async}, \ldots) decides what syntax is appropriate. We can only provide 
anecdotal recommendation -- for computations where the \emph{monoidal} interface is more important, 
the \ident{for}/\ident{yield} notation fits better, while for computations where
the \emph{monadic} interface dominates we prefer \ident{let!} and \ident{return}.

% --------------------------------------------------------------------------------------------------

\subsection{Layered asynchronous sequences}
\label{sec:intro-asyncseq}

It is often useful to combine non-standard aspects of multiple computation types. Abstractly,
this has be described using monad transformers [99]. F\# does not support monad transformers directly, 
but they provide a useful conceptual framework. For example, we migth combine 
non-blocking execution of asynchronous workflows with the ability to return multiple results in 
list comprehensions -- a file download can then produce data in 1kB buffers as they become available. 
Such computation is captured by \emph{asynchronous sequences} [14].

Assuming $\ident{Async}\,\tau$ is the type of asynchronous workflows, the composed
computation can be expressed as follows (inspired by the list transformer [99]):

\begin{equation*}
\begin{array}{lcl}
\kvd{type}~\ident{AsyncSeqInner}\,\tau &~=~& \ident{AsyncNil} ~|~
    \ident{AsyncCons}~\kvd{of}~\tau \times \ident{Async}\,\tau \\
\kvd{type}~\ident{AsyncSeq}\,\tau &=& \ident{Async}\,(\ident{AsyncSeqInner}\,\tau)
\end{array}
\end{equation*}

When provided with a continuation, asynchronous sequence calls it with either \ident{AsyncNil}
(to denote the end of the sequence) or with \ident{AsyncCons} that carries a value, together with
the rest of the asynchronous sequence. It turns out that the flexibility of computation expression 
makes it possible to provide an elegant syntax for writing computations of this type:
%
\begin{equation*}
\begin{array}{l}
\kvd{let rec}~\ident{urlPerSecond}~n = \ident{asyncSeq}~\{ \\
\quad \kvd{do!}~\ident{Async}.\ident{Sleep}~1000 \\
\quad \kvd{yield}~\ident{getUrl}~i \\
\quad \kvd{yield!}~\ident{iterate}~(i+1) ~\}
\end{array}
\qquad
\begin{array}{l}
\kvd{let}~\ident{pagePerSecond}~urls = \ident{asyncSeq}~\{ \\
\quad \kvd{for}~url~\kvd{in}~\ident{urlPerSecond}~0~\kvd{do}\\
\quad\quad \kvd{let!}~html = \ident{asyncFetch}~url \\
\quad\quad \kvd{yield}~url, html ~\}
\end{array}
\end{equation*}
%
The \ident{urlPerSecond} function creates an asynchronous sequence that produces one URL per
second. It uses bind (\ident{do!}) of the asynchronous workflow monad to wait one second
and then composition of asynchronous sequences, together with \ident{yield} to produce the 
next URL. The \ident{pagePerSecond} function uses \ident{for} to iterate over (bind on) an
asynchronous sequence and then \ident{let!} to wait for (bind on) an asynchronous workflow.
The \ident{for} loop is asynchronous and lazy -- it is run each time the caller asks for the 
next result.

Asynchronous sequences form a monad and so we could use the standard notation for monads with
just \ident{let!} and \ident{return}. We would then need explicit lifting function that turns
an asynchronous workflow into an asynchronous sequence that returns a single value. However,
F\# computation expressions allow us to do better. We can define both \ident{For} and 
\ident{Bind} with the following types:
%
\begin{equation*}
\begin{array}{rcll}
\ident{asyncSeq}.\ident{For} &~:~& \ident{AsyncSeq}\,\alpha &
   \rightarrow (\alpha \rightarrow \ident{AsyncSeq}\,\beta) \rightarrow \ident{AsyncSeq}\,\beta\\
\ident{asyncSeq}.\ident{Bind} &~:~& \ident{Async}\,\alpha   &
   \rightarrow (\alpha \rightarrow \ident{AsyncSeq}\,\beta) \rightarrow \ident{AsyncSeq}\,\beta\\
\end{array}
\end{equation*}
%
We omit the translation of the above example -- it is a straightforward variation on what we have 
seen so far. A more important point is that we can again benefit from the fact that operations
of the computation builder are not restricted to a specific type (such as \emph{Bind} for some
monad $M$).

As previously, the choice of the syntax is left to the author of the computation. Here, asynchronous
sequences are an additive monad and so we use \ident{for}/\ident{yield}. Underlying
asynchronous workflows are just monads, so it makes sense to add \ident{let!} that automatically
lifts a workflow to an asynchronous sequence.

An important aspect of realization that asynchronous sequences can be described using a monad
transformer means that certain laws hold. In \S\ref{sec:laws-transf} we show how these map to the 
computation expression syntax.

% --------------------------------------------------------------------------------------------------

\subsection{Applicative formlets}
\label{sec:intro-formlets}

Our last example uses a computation based on \emph{applicative functors} [2], which is a weaker
(and thus more common) abstraction than monads. The difference between applicative and monadic
computations is that monadic computation can perform different effects depending on values
obtained earlier during the computation. On the other hand, the structure of effects of applicative
computation is fully determined by its structure. 

In other words, it is not possible to choose which computation to run (using \ident{let!} or 
\ident{do!}) based on values obtained in previous \ident{let!} bindings. The following example
demonstrates this using a web form abstraction called formlets [99]:
%
\vspace{-0.3em}
\begin{equation*}
\begin{array}{l}
\kvd{let}~\ident{userFormlet} = \ident{formlet}~\{\\
\quad \kvd{let!}~name = \ident{Formlet}.\ident{textBox} \\
\quad \kvd{and}~gender = \ident{Formlet}.\ident{dropDown}~[\texttt{"Male"}; \texttt{"Female"}] \\
\quad \kvd{return}~name + \texttt{" "} + gender ~\}
\end{array}
\vspace{-0.3em}
\end{equation*}
%
The computation describes two aspects -- the rendering and the processing of entered values.
The rendering phase uses the fixed structure to produce HTML with text-box and drop-down elements.
In the processing phase, the values of \emph{name} and \emph{gender} are available and are used to
calculate the result of the form.

The structure of the form needs to be known without having access to specific values. The syntax
uses parallel binding (\kvd{let!}\ldots\kvd{and}\ldots), which binds a fixed number of independent
computations. The rest of the computation cannot contain other (applicative) bindings.

There are two equivalent ways of defining applicative functors. We use the less common style
which uses two operations. \emph{Merge} of type $F\alpha \rightarrow F\beta \rightarrow F(\alpha \times \beta)$
represents composition of structure (without any knowledge of specific values) and \emph{Map} of 
type $F\alpha \rightarrow (\alpha \rightarrow \beta) \rightarrow F\beta$ transforms the (pure) value.
The computation expression from the previous example is translated as follows:
%
\vspace{-0.3em}
\begin{equation*}
\begin{array}{l}
\ident{formlet}.\ident{Map} \\
\quad (~\ident{formlet}.\ident{Merge}(\ident{Formlet}.\ident{textBox}, 
  \ident{Formlet}.\ident{dropDown}~[\texttt{"Male"}; \texttt{"Female"}]),  \\
\quad~~\kvd{fun}~(name, gender) \rightarrow name + \texttt{" "} + gender~)
\end{array}
\vspace{-0.3em}
\end{equation*}
%
The parallel binding is turned into an expression that combines all bindings using the 
\emph{Merge} operation. This part of the computation defines the structure and formlets use it
for rendering HTML. The rest of the computation is turned into a pure function passed to \emph{Map}.
Note that the translation allows uses beyond applicative functors. The \ident{let!}\ldots\ident{and}\ldots
syntax can be used with monads to write zip comprehensions [99], which are useful for
parsing, parallelism and more [99].

Applicative functors were first introduced to support \emph{applicative} programming style where
monads are not needed. The \emph{idiom brackets} notation [99] fits that purpose better. We
find that computation expressions provide a useful alternative for more complex code and fit better 
with the impure nature of F\#.

% ==================================================================================================

\section{Semantics of computation expressions}
\label{sec:semantics}

The F\# language specification [17] documents computation expressions as a purely syntactic 
mechanism. They are desugared before type-checking, which is then performed on the translated code 
using standard F\# typing rules. Similarly to Haskell's rebindable syntax [99], this provides more
flexibility and allows the users to invent previously unforseen abstractions.

In this paper, we look at computation expressions from a different perspective. We relate them to 
standard abstract computation types. In \S\ref{sec:semantics-typing}, we present a new typing that 
captures such common uses and would make the system more robust by supporting better error messages 
and disallowing non-standard uses.

% --------------------------------------------------------------------------------------------------

\subsection{Syntax}
\label{sec:semantics-syntax}

The full syntax of computation expressions is given in the language specification, but the following
lists all important constructs that we consider in this paper:
%
\begin{equation*}
\begin{array}{lclcl}
\expr  &=& \expr ~\{~ \cexpr ~\}                       &\quad&\textnormal{(computation expression)}\\
\binds &=& v=\expr                                          &&\textnormal{(single binding)}\\
       &|& v=\expr ~\kvd{and}~\binds                        &&\textnormal{(parallel binding)}\\
\cexpr &=& \kvd{let}~v=\expr~\kvd{in}~\cexpr                &&\textnormal{(binding value)} \\
       &|& \kvd{let!}~\binds~\kvd{in}~\cexpr                &&\textnormal{(binding computation)} \\
       &|& \kvd{for}~v~\kvd{in}~\expr~\kvd{do}~\cexpr       &&\textnormal{(for loop computation)} \\
       &|& \kvd{return}~\expr                               &&\textnormal{(return value)} \\
       &|& \kvd{return!}~\expr                              &&\textnormal{(return computation)} \\
       &|& \kvd{yield}~\expr                                &&\textnormal{(yield value)} \\
       &|& \kvd{yield!}~\expr                               &&\textnormal{(yield computation)} \\
       &|& \cexpr_1; \cexpr_2                               &&\textnormal{(compose computations)} \\       
       &|& \expr                                            &&\textnormal{(effectful expression)} \\
\end{array}
\end{equation*}
%
We do not include \ident{do!} which can be easily expressed in terms of the \ident{let!} construct. 
To accommodate the applicative syntax, we use a syntactic category \binds\; to express one or more
variable bindings. 

For space reasons, we also omit imperative \ident{while} and exception handling 
constructs. Both of these are an important part of computation expressions. The design principle is 
that the user should be able to wrap any valid F\# code in a computation block and augment it with 
non-standard computational aspect, while preserving the semantics (including exception handling).

% --------------------------------------------------------------------------------------------------

\begin{figure}[t!]
\begin{equation*}\hspace{-2em}
\begin{array}{rl}
\multicolumn{2}{c}{
  \boxed{\Gamma \vdash \expr :\tau}\quad\textnormal{and}\quad
    \boxed{\Gamma \bvdash \binds : \mtyp\Sigma}
}\\[1.5em]
%
(\textnormal{run})~
\inference
  { \Gamma \vdash \expr : \sigma  & \Gamma \cvdash \cexpr : \mtyp{\tau} }
  { \Gamma \vdash \expr ~\{~ \cexpr ~\} : \ntyp \tau }
&\quad\begin{array}{l}
(\forall \alpha: \sigma.\plc{Run}~:~\dtyp{\alpha} \rightarrow \ntyp \alpha \\
\;\,\forall\alpha: \sigma.\plc{Delay} : (\unit \rightarrow \mtyp{\alpha}) \rightarrow \dtyp{\alpha} )
\end{array}
\\[1.5em]
(\textnormal{bind-one})~
\inference 
  { \Gamma \vdash \expr : \mtyp{\tau} }
  { \Gamma \bvdash v=\expr : \mtyp{(\tya{v}{\tau})} }
\\[1.5em]
(\textnormal{bind-par})~
\inference 
  { \Gamma \vdash \expr : \tau & \Gamma \bvdash \binds : \mtyp{\Sigma} }
  { \Gamma \bvdash v=\expr~\kvd{and}~\binds : \mtyp{(\Sigma, \tya{v}{\tau})}  }
&\quad\begin{array}{l}
(\forall\alpha,\beta: \sigma.\plc{Merge}~:\\
\quad \mtyp{\alpha} \rightarrow \mtyp{\beta} \rightarrow \mtyp{(\alpha \times \beta)} )  
\end{array}
\\[1.5em]
%
\multicolumn{2}{c}{
  \boxed{\Gamma \cvdash \cexpr : \mtyp\tau}
}\\[1.5em]
%
(\textnormal{let})~
\inference
  { \Gamma \vdash \expr : \tau_1 &
    \Gamma, \tya{v}{\tau_1} \cvdash \cexpr : \mtyp{\tau_2} }
  { \Gamma \cvdash \kvd{let}~v=\expr~\kvd{in}~\cexpr : \mtyp{\tau_2}  }
\\[1.5em]
(\textnormal{bind})~
\inference
  { \Gamma \bvdash \binds : \mtyp\Sigma &
    \Gamma, \Sigma \cvdash \cexpr : \ntyp{\tau} }
  { \Gamma \cvdash \kvd{let!}~\binds~\kvd{in}~\cexpr : \ntyp{\tau}  }
&\quad\begin{array}{l}
(\forall\alpha,\beta: \sigma.\plc{Bind}~:\\
\quad \mtyp{\alpha} \rightarrow (\alpha \rightarrow \ntyp{\beta}) \rightarrow \ntyp{\beta} )  
\end{array}
\\[1.5em]
(\textnormal{map})~
\inference
  { \Gamma \bvdash \binds : \mtyp\Sigma &
    \Gamma, \Sigma \vdash \expr : \tau }
  { \Gamma \cvdash \kvd{let!}~\binds~\kvd{in}~\kvd{return}~\expr : \ntyp{\tau}  }
&\quad\begin{array}{l}
(\forall\alpha,\beta: \sigma.\plc{Map}~:\\
\quad \mtyp{\alpha} \rightarrow (\alpha \rightarrow \beta) \rightarrow \ntyp{\beta} )  
\end{array}
\\[1.5em]
(\textnormal{for})~
\inference
  { \Gamma \vdash \expr : \mtyp{\tau_1} &
    \Gamma, \tya{v}{\tau_1} \cvdash \cexpr : \ntyp{\tau_2} }
  { \Gamma \cvdash \kvd{for}~v~\kvd{in}~\expr~\kvd{do}~\cexpr : \ntyp{\tau_2}  }
&\quad\begin{array}{l}
(\forall\alpha,\beta: \sigma.\plc{For}~:\\
\quad \mtyp{\alpha} \rightarrow (\alpha \rightarrow \ntyp{\beta}) \rightarrow \ntyp{\beta} )  
\end{array}
\\[1.5em]
(\textnormal{return-val})~
\inference
  { \Gamma \vdash \expr : \tau }
  { \Gamma \cvdash \kvd{return}~\expr : \mtyp{\tau}  }
&\quad(\forall\alpha: \sigma.\plc{Return}~:~\alpha \rightarrow \mtyp{\alpha})
\\[1.5em]
(\textnormal{return-comp})~
\inference
  { \Gamma \vdash \expr : \mtyp \tau }
  { \Gamma \cvdash \kvd{return!}~\expr : \ntyp{\tau}  }
&\quad(\forall\alpha: \sigma.\plc{ReturnFrom}~:~\mtyp{\alpha} \rightarrow \ntyp{\alpha})  
\\[1.5em]
(\textnormal{seq})~
\inference
  { \Gamma \cvdash \cexpr_1 : \mtyp{\tau_1}  &  \Gamma \cvdash \cexpr_2 : \ntyp{\tau_2}}
  { \Gamma \cvdash \cexpr_1; \cexpr_2 : \ltyp{\tau_1}  }
&\quad\begin{array}{l}
(\forall\alpha: \sigma.\plc{Delay} ~:~ (\unit \rightarrow \ntyp{\alpha}) \rightarrow \dtyp{\alpha} \\
\;\,\forall\alpha: \sigma.\plc{Combine} : \mtyp{\tau_1} \rightarrow \dtyp{\alpha} \rightarrow \ltyp{\alpha} )
\end{array}
\\[1.5em]
(\textnormal{zero})~
\inference
  { \Gamma \vdash \expr : \unit }
  { \Gamma \cvdash \expr : \mtyp{\tau}  }
&\quad(\sigma.\plc{Zero}~:~\unit \rightarrow \mtyp{\tau})
\end{array}
\end{equation*}
\vspace{-1em}
\caption{Typing rules for computation expressions}
\label{fig:typing}
\vspace{-1em}
\end{figure}

% --------------------------------------------------------------------------------------------------

\subsection{Typing}
\label{sec:semantics-typing}

The typing rules in Figure~\ref{fig:typing} are written using three judgments. Standard F\# 
expressions are typed using $\Gamma \vdash \expr : \tau$. Computation expressions always return
computation of type $\mtyp{\tau}$ and are typed using $\Gamma \cvdash \cexpr : \mtyp{\tau}$.
Finally, we use a helper judgement $\Gamma \bvdash \binds : \mtyp{\Sigma}$ to check bindings
of multiple computations. The judgement produces a variable context with newly bound variables, 
wrapped in the type $M$ of the bound computations.

The latter two are parameterized by the type of the computation expression builder (such as
\ident{seq} or \ident{async}). The operations supported by the builder determine which syntactic
constructs are enabled. Typing rules that require a certain operation have a side-condition
on the right, which specifies the requirement.

In most of the side-conditions, the functions are universally quantified over the type of values
(written as $\alpha, \beta$). This captures the fact that computation should not restrict the 
values that users can work with. However, this is not the case in the rules (\emph{seq}) and 
(\emph{zero}). Here, we can only require that a specific instantiation is available -- the reason 
is that these operations may be used in two different ways. As discussed in \S\ref{sec:intro-async}, 
for monads the result of \emph{Zero} and the first argument of \emph{Combine} are restricted to
$M\,\ident{unit}$. They can be universally quantified only if the computation is monoidal 
\S\ref{sec:intro-seq-parsers}.

Another notable aspect of the typing is that a single computation expression may use multiple
computation types (written $M, N, L$ and $D$). In \emph{Bind} and \emph{For}, the type of bound
argument is $M$, but the resulting computation is $N$ (we require that bind returns the same
type of computation as the one produced by the function). This corresponds to the typing used
by computations arising from monad transformers \S\ref{sec:intro-asyncseq}. Although combining
multiple computation types is not as frequent, computations often have a delayed version which
we write as $D$. This is an important consideration for impure langauges such as F\# 
\S\ref{sec:semantics-delayed}.

Finally, we omitted typing for \ident{yield} and \ident{yield!} because it is similar to the 
typing of \ident{return} and \ident{return!} (using \emph{Yield} and \emph{YieldFrom} operations,
respectively).

% --------------------------------------------------------------------------------------------------

\begin{figure}[t]
\begin{equation*}
\hspace{-1em}\begin{array}{rcl}
\expr ~\{~ \cexpr ~\} &~=~& \kvd{let}~m = \expr ~\kvd{in}~m.\ident{Run}(m.\ident{Delay}(\kvd{fun}~() \rightarrow \tsl{\cexpr}))
\\[0.5em]
\tsl{\kvd{let}~v=\expr~\kvd{in}~\cexpr}          &=& \kvd{let}~v=\expr~\kvd{in}~\tsl{\cexpr} \\[0.08em]
\tsl{\kvd{let!}~\binds~\kvd{in}~\cexpr}          &=& m.\ident{Bind}(\tsb{\binds},\kvd{fun}~\tsv{\binds}\rightarrow \tsl{\cexpr}) \\[0.08em]
\tsl{\kvd{let!}~\binds~\kvd{in}~\kvd{return}\expr}          &=&
   m.\ident{Map}(\tsb{\binds},\kvd{fun}~\tsv{\binds}\rightarrow \expr) \\[0.08em]
\tsl{\kvd{for}~v~\kvd{in}~\expr~\kvd{do}~\cexpr} &=& m.\ident{For}(\expr,\kvd{fun}~()\rightarrow \tsl{\cexpr}) \\[0.08em]
%
\tsl{\kvd{return}~\expr}   &=& m.\ident{Return}(\expr) \\[0.08em]
\tsl{\kvd{return!}~\expr}  &=& m.\ident{ReturnFrom}(\expr) \\[0.08em]
\tsl{\cexpr_1; \cexpr_2}   &=& m.\ident{Combine}(\tsl{\cexpr_1}, \;m.\ident{Delay}(\kvd{fun}~() \rightarrow \tsl{\cexpr_2} )) \\[0.08em]
\tsl{\expr}                &=& \expr; ~m.\ident{Zero}()
\\[0.5em]
\tsb{v=\expr} &=& \expr \\[0.08em]
\tsb{v=\expr ~\kvd{and}~\binds} &=& m.\ident{Merge}(\expr, \tsl{\binds})
\\[0.5em]
\tsv{v=\expr} &=& v \\[0.08em]
\tsv{v=\expr ~\kvd{and}~\binds} &=& v, \tsv{\binds} 
\end{array}
\end{equation*}
\vspace{-1.2em}
\caption{Translation rules for computation expressions}
\label{fig:translation}
\vspace{-1em}
\end{figure}

% --------------------------------------------------------------------------------------------------

\subsection{Translation}
\label{sec:semantics-transl}

The translation is defined as a mapping $\tsl{-}$ that is parameterized by a variable $m$ which
refers to the current instance of a computation builder. This parameter is later used in the
translation to invoke members of the builder, such as $m.\ident{Return}(\ldots)$. Multiple 
variable bindings are translated using $\tsb{\binds}$ and we define a helper mapping 
$\tsv{\binds}$ that turns bindings into a simple pattern that can be used to decompose a tuple
constructed by merging computations using the \emph{Merge} operation.

According to the F\# specification, a particular construct of computation expression syntax is
allowed only when the static type of the computation builder defines members that are required
by the translation. It is easy to check that our typing rules guarantee that a well-typed computation
expression can always be translated to a well-typed F\# expression.

Careful readers have already noticed that our definition of $\tsl{-}$ is ambiguous. The 
\ident{let!} binding followed by \ident{return} can be translated in two different ways. In the
real implementation, the translation using \emph{Map} is preferred, but we do not specify this in
the paper. The reason is that the laws in \S\ref{sec:laws-monads} require the two translations to
be equivalent. For monads, this equivalence is easy to see by considering the definition of
\emph{Map} in terms of \emph{Bind} and \emph{Return}.

In earlier discussion, we omitted the \emph{Run} and \emph{Delay} members in the translation of
$\expr~\{\,\cexpr\,\}$. The next section discusses these two in more details.

% --------------------------------------------------------------------------------------------------

\subsection{Delayed computations}
\label{sec:semantics-delayed}

We already mentioned that side-effects are an important consideration when adding sequencing to
monadic comptuations \S\ref{sec:intro-async}. In effectful languages, we need to distinguish 
between two types of monads. We use the term \emph{monadic computation} for monads that represent 
a delayed computation such as asynchronous workflows or lazy list comprehensions; the term 
\emph{monadic containers} will be used for monads that represent a wrapped non-delayed value
(such as the option type, non-lazy list or the identity monad).

\vspace{-1em}
\subsubsection{Effects and monadic computations.} The defining feature of \emph{monadic computations}
is that they permit a \emph{Delay} operation of type $(\unit \rightarrow M\alpha) \rightarrow M\alpha$
that does not perform the effects associated with the function used as an argument.
For example, in the continuation monad (underlying asynchronous workflows), the operation builds 
a computation that takes a continuation -- and so the effects are only run when the continuation
is provided.

Before going further, we revist the translation of asynchronous workflows using the full set of
rules to show how \emph{Run} and \emph{Delay} are used. Consider the the following simple computation
with a corresponding translation:
%
\begin{equation*}
\begin{array}{l}
\kvd{let}~\ident{answer} = \ident{async}~\{\\
\quad \ident{printfn}~ \texttt{"Welcome..."}\\
\quad \kvd{return}~ 42 ~\}
\end{array}
\qquad
\begin{array}{l}
\kvd{let}~\ident{answer} = \ident{async}.\ident{Run}(\ident{async}.\ident{Delay}(\kvd{fun}~()\rightarrow\\
\quad \ident{printfn}~ \texttt{"Welcome..."}\\
\quad \ident{async}.\ident{Return}(42)~))
\end{array}
\end{equation*}
%
For monadic computations such as asynchronous workflows, we do not expect that the defining
\ident{answer} will print ``Welcome''. This is achieved by the wrapping specified in the 
translation rule for the $\expr~\{\,\cexpr\,\}$ expression. As already mentioned, the result
of \emph{Delay} is a 

In this case, the result of \emph{Delay} is a computation $A\,\ident{int}$ that encapsulates the
delayed effect. For monadic containers, the \emph{Run} function is a simple identity -- contrary
to what the name suggests, it does not run the computation (although that might be an interesting
use beyond standard abstract computations). The need for \emph{Run} becomes obvious when we look
at monadic containers.

\vspace{-1em}
\subsubsection{Effects and monadic containers.} For monadic containers, it is impossible to define
a \emph{Delay} operation that does not perform the effects and has a type $(\unit \rightarrow M\alpha) \rightarrow M\alpha$,
because the resulting type has no way of capturing unevaluated code. However, the (\emph{seq}) typing 
rule in Figure~\ref{fig:typing} permits an alternative typing. Consider the following example using 
the Maybe (option) monad:
%
\begin{equation*}
\begin{array}{l}
\ident{maybe}~\{~\kvd{if}~b=0~\kvd{then}~\kvd{return!}~\ident{None}\\
\hspace{3.85em}  \ident{printfn}~\texttt{"Calculating..."}\\
\hspace{3.85em}  \kvd{return}~a\,/\,b~\}\\
\end{array}
\end{equation*}
%
Using the same translation rules, \emph{Run}, \emph{Delay} and \emph{Delay} are inserted as follows:
%
\begin{equation*}
\begin{array}{l}
\ident{maybe}.\ident{Run}(\ident{maybe}.\ident{Delay}(\kvd{fun}~()\rightarrow \ident{maybe}.\ident{Combine}\\
\quad\quad (~(\kvd{if}~b=0~\kvd{then}~\ident{maybe}.\ident{ReturnFrom}(\ident{None})~
    \kvd{else}~\ident{maybe}.\ident{Zero}()), \\
\hspace{2.9em} \ident{maybe}.\ident{Delay}(\kvd{fun}~()\rightarrow \ident{printfn}~\texttt{"Calculating..."}\\
\hspace{12.8em}     \ident{maybe}.\ident{Return}(a\,/\,b))~)~))
\end{array}
\end{equation*}

\noindent
The key idea is that we do not have to use the type $M\alpha$ for representing delayed computations,
but can instead use two different types throughout the code. $M\alpha$ for values representing
evaluated containers and $\unit \rightarrow M\alpha$ for delayed computations. The operations 
have the following types:
%
\begin{equation*}
\begin{array}{lcl}
\emph{Delay}   &~:~& (\unit \rightarrow M\alpha) \rightarrow (\unit \rightarrow M\alpha)\\
\emph{Run}     &~:~& (\unit \rightarrow M\alpha) \rightarrow M\alpha\\
\emph{Combine} &~:~& M\,\unit \rightarrow (\unit \rightarrow M\alpha) \rightarrow M\alpha
\end{array}
\end{equation*}
%
Here, the \emph{Delay} operation becomes just an identity that returns the function created by the
translation. In the translation, the result of \emph{Delay} can be passed either to \emph{Run}
or as the second argument of \emph{Delay}, so these need to be changed accordingly. The \emph{Run}
function now becomes important as it turns the delayed function into a value of the expected
type $M\alpha$.

\vspace{-1em}
\subsubsection{Unified treatment of effects.} In the typing rules \S\ref{sec:semantics-typing}, 
we did not explicitly list the two options, because they can be generalized. We require that the
result of \emph{Delay} is some (possibly different) abstract type $D\alpha$ representing delayed
computations. For monadic computations, the type is just $M\alpha$ and for monadic containers,
it is $\unit \rightarrow M\alpha$. Our typing is even more flexible, as it allows usage of 
multiple different computation types -- but treatment of effects is one example where this
additional flexibility is necessary.

Finally, it should be noted that we used a slight simplification. The actual F\# implementation 
does not strictly require \emph{Run} and \emph{Delay} in the translation of $\expr~\{\,\cexpr\,\}$. 
They are only used if they are present. 


% ==================================================================================================

\section{Computation expression laws}
\label{sec:laws}

Although computation expressions are not tied to any specific abstract computation type, we 
showed that they are usually used with well-known abstractions like monads, monad transofrmers or 
applicative functors. 

This means three good things. First, we get better understanding of what computations can be encoded
(and how). Second, we can add a more precise typing \S\ref{sec:semantics-typing}. Third, we know
that certain syntactic transformations (refactorings) preserve the meaning of computation. This
section looks at the last point. 

This section assumes that there are no side-effects and we ignore \emph{Run} and \emph{Delay}.
These can be added to the picture, but it complicates the presentation.

% --------------------------------------------------------------------------------------------------

\subsection{Monoid and semi-group laws}
\label{sec:laws-monoids}

We start by looking at the simplest possible structure. A semigroup $(S, \circ)$ consists of a set
$S$ and an associative binary operation $\circ$ meaning that $a \circ (b \circ c) = (a \circ b) \circ c$.
A computation expression corresponding to a semigroup defines only \emph{Combine} (of a type
$M\alpha \rightarrow M\alpha \rightarrow M\alpha$). To allow appropriate syntax, we also add
\emph{YieldFrom} which is just the identity function. The associativity implies the following
syntactic equivalence:
%
\begin{equation*}
\ident{m}~\{~\cexpr_1; \,\cexpr_2; \,\cexpr_3 ~\} ~\equiv~
  \ident{m}~\{~\kvd{yield!}~\ident{m}~\{ \cexpr_1; \,\cexpr_2; \}; \,\cexpr_3 ~\}
\end{equation*}

\noindent
For semigroups, the syntax is rather limited, but given a value $x$ of type $M\tau$ it is possible
to write $\ident{yield!}~x$ to return the value. 

A monoid $(S, \circ, \epsilon)$ is a semigroup $(S, \circ)$ with an identity element $\epsilon$ meaning 
that for all values $a\in S$ it holds that $\epsilon \circ a = a = a \circ \epsilon$. The identity
element can be added to computation builder as the \emph{Zero} member. This operation is used when
a computation uses conditional without \ident{else} branch. Thus we get:
%
\begin{equation*}
\begin{array}{l}
\ident{m}~\{~  \kvd{if}~\ident{false}~\kvd{then}~ \cexpr_1\\
\hspace{1.9em} \cexpr_2 ~\}
\end{array} 
~\equiv~
\ident{m}~\{~ \cexpr_2 ~\}\\
~\equiv~
\begin{array}{l}
\ident{m}~\{~  \cexpr_2 \\
\hspace{2.0em} \kvd{if}~\ident{false}~\kvd{then}~ \cexpr_1 ~\}
\end{array}
\end{equation*}
%
Although these are simple laws, they can be used to reason about list comprehensions. The associativity
means that we can move a part of computation expression (that uses \ident{yield!} repeatedly) into a
separate computation. To use the identity law, consider a recursive function that generates numbers
up to 100:
%
\begin{equation*}
\begin{array}{l}
\kvd{let}~\kvd{rec}~\ident{range}~n = \\
\quad \ident{seq}~\{~ \kvd{yield}~n\\
\hspace{3.7em} \kvd{if}~n < 100~\kvd{then}~\kvd{yield!}~\ident{range}~(n + 1)~\}
\end{array}
\end{equation*}
%
Here, we can see that when $n=100$, the body is equivalent to just $\ident{m}~\{~\ident{yield}~100~\}$.
Indeed, this is an expected property of the \ident{if} construct -- the law guarantees that the 
property holds even for an \ident{if} construct that is reinterpreted by some (monoidal) computation
expression.

% --------------------------------------------------------------------------------------------------

\subsection{Monad and additive monad laws}
\label{sec:laws-monads}

Monad laws are well-understood and the corresponding equivalent computation expressions do
not significantly differ from the laws about Haskell's do notation:
%
\begin{equation*}
\begin{array}{rl}
\ident{m}~\{~ \kvd{let!}~y=\ident{m}~\{~\kvd{return}~x~\}~\kvd{in}~\cexpr~ \}
&~\equiv~ \ident{m}~\{~ \kvd{let}~y=x~\kvd{in}~\cexpr~ \}
\\
\ident{m}~\{~ \kvd{let!}~x=c~\kvd{in}~\kvd{return}~x \}
&~\equiv~ \ident{m}~\{~ \kvd{return!}~c \}
\end{array}
\end{equation*}
\begin{equation*}
\begin{array}{rcl}
      &\ident{m}~\{~ \kvd{let!}~x=\ident{m}~\{~\kvd{let!}~y=c~\kvd{in}~\cexpr_1~\}~\kvd{in}~\cexpr_2~\}&\equiv\\
\equiv&\ident{m}~\{~ \kvd{let!}~y=c~\kvd{in}~\kvd{let!}~x=\ident{m}~\{~\cexpr_1~\}~\kvd{in}~\cexpr_2~\}\\
\end{array}
\end{equation*}
%
However, there is more to be said about the \emph{Map} operation in the translation and
about laws of additive monads (\ident{MonadPlus} typeclass in Haskell).

\vspace{-1em}
\subsubsection{Alternative translations.} When discussing the translation rules 
\S\ref{sec:semantics-transl}, we noted that the rules are ambiguous when both \emph{Map} and
\emph{Bind} operations are present. The following can be translated both monadically and 
applicatively:
%
\begin{equation*}
\ident{m}~\{~\kvd{let!}~x = c~\kvd{in}~\kvd{return}~\expr~\}\\
\end{equation*}
%
The two translations are shown below. Assuming that our computation is a monad,
this is a well-known definition of \emph{Map} in terms of \emph{Bind} and \emph{Return}:
%
\begin{equation*}
\ident{m}.\ident{Map}(x, \kvd{fun}~x \rightarrow \expr) ~\equiv~
  \ident{m}.\ident{Bind}(x, \kvd{fun}~x \rightarrow \ident{m}.\ident{Return}(\expr))\\
\end{equation*}
%
More generally, if a computation builder defines both \emph{Map} and \emph{Bind} (even if they are
not based on a monad), we require this equation to guarantee that the two possible translations 
produce equivalent computations.

\subsubsection{Additive monads.} Additive monads are computations that combine monad with 
the monoidal structure. As shown earlier \S\ref{sec:intro-seq-parsers}, these can be embedded
using \ident{let!}/\ident{return} or using \ident{for}/\ident{yield} (depending on which 
aspect is ``more important'').

The set of laws required for such computations is not fully resolved [99]. A more generally 
accepted law is \emph{left distributivity} -- applying monoidal operation and then binding is equivalent
to binding on two computations and then combining the results. In terms of computation builder
operations:
%
\begin{equation*}
\ident{m}.\ident{For}(\ident{m}.\ident{Combine}(a, b), f)
~\equiv~
\ident{m}.\ident{Combine}(\ident{m}.\ident{For}(a, f), \ident{m}.\ident{For}(b, f))
\end{equation*}
%
We intentionally use the \emph{For} operation (corresponding to the \ident{for} keyword), because
this leads to the following intuitive syntactic equality:
%
\begin{equation*}
\begin{array}{l}
\ident{m}~\{~  \kvd{for}~x~\kvd{in}~\ident{m}~\{~\cexpr_1;\;\cexpr_2~\}~\kvd{do}\\
\hspace{2.0em}\quad \cexpr~\}
\end{array}~
\begin{array}{c}\equiv\\~\end{array}~
\begin{array}{l}
\ident{m}~\{~ \kvd{for}~x~\kvd{in}~\ident{m}~\{~\cexpr_1~\}~\kvd{do}~\cexpr \\
\hspace{2em}  \kvd{for}~x~\kvd{in}~\ident{m}~\{~\cexpr_2~\}~\kvd{do}~\cexpr~\}
\end{array}
\end{equation*}
%
If we read the code as an imperative looping construct (without the computational reinterpretation),
then this is, indeed, a valid law about \ident{for} loops.

Another law that is sometimes required about additive monads is \emph{left catch}. It states that
combining a computation that immediately returns a value with any other computation results in 
a computation that just returns the value:
%
\begin{equation*}
\hspace{-3em}
\ident{m}.\ident{Combine}(\ident{m}.\ident{Return}(v), a)
~~\equiv~~
\ident{m}.\ident{Return}(v)
\end{equation*}
%
This time, we intentionally used the \emph{Return} member instead of \emph{Yield}, because the law
corresponds to the following syntactic equivalence:
%
\begin{equation*}
\hspace{-3em}
\begin{array}{l}
\ident{m}~\{~  \kvd{return}~v; ~ \cexpr~\}
\end{array}~~\equiv~~
\begin{array}{l}
\ident{m}~\{~  \kvd{return}~v~\}\\
\end{array}
\end{equation*}
%
The fact that \emph{left distributivity} corresponds to an intuitive syntactic equality
about \ident{for}/\ident{yield} while \emph{left catch} corresponds to a syntactic equality
about \ident{let!}/\ident{return} provides a useful guidance for choosing between the two
syntactic options. The former is appropriate for list comprehensions (and other collections), 
while the latter is appropriate for example for the left-biased option (Maybe) monad, imperative 
computations [99] or software transactional memory.

\subsection{Monad transformers}
\label{sec:laws-transf}

There are multiple ways of composing or layering monads [99, 98]. Monad transformers are perhaps 
the most widely known technique. A monad transformer is a type constructor $T\,m$ together with
a \emph{Lift} operation. For some monad $M$ the operation has a type $M\,\alpha \rightarrow T\,M\,\alpha$.
and it turns a computation in the underlying monad into a computation in the composed monad.

The result of monad transformer is also a monad. This means that we can use the usual syntactic 
sugar for monads, such as the do notation in Haskell. However, a more specific notation can
use the additional \emph{Lift} operation.

We demonstrated encoding of syntax for composed monads when discussing
asynchronous sequences \S\ref{sec:intro-asyncseq}. An asynchronous sequence $\ident{AsyncSeq}\,\alpha$ 
is a computation obtained by applying the list monad transformer [97] to the asynchronous
workflow $\ident{Async}\,\alpha$ monad. Asynchronous sequences are \emph{additive monads} satisfying 
the left distributivity law, so we choose the \ident{for}/\ident{yield} syntax for working with 
the composed computation. We also provided an additional \emph{Bind} operation to support 
awaiting a single asynchronous workflow using the \ident{let!} construct. This operation is
defined in terms of \emph{Lift} of the monad transformer and \emph{For} (monadic bind) of the
composed computation:
%
\begin{equation*}
\ident{asyncSeq}.\ident{Bind}(a, f) = \ident{asyncSeq}.\ident{For}(\ident{asyncSeq}.\ident{Lift}(a), f)
\end{equation*}
%
There are two laws that hold about monad transforers. To avoid confusion, we use asynchronous 
workflows and sequences in the explanation, but we could easily generalize. The first law states 
that composing \emph{Return} of asynchronous workflows with \emph{Lift} should be equivalent to the 
\emph{Yield} of asynchronous sequences. The other states that \emph{Lift} distributes over monadic
bind. 

Our syntax always combines \emph{Lift} with \emph{For} and so there are multiple syntactic 
equivalences that follow from the laws. The following are the most direct (the first one also
relies on right identity for monads):
%
\begin{equation*}
\begin{array}{c}
\ident{asyncSeq}~\{~ \kvd{let!}~x = \ident{async}~\{~\kvd{return}~v~\}~\kvd{in}~\kvd{return}~x ~\} ~\equiv~
  \ident{asyncSeq}~\{~ \kvd{return}~v ~\}
\\[0.7em]
\ident{asyncSeq}~\{~ \kvd{let!}~x = \ident{async}~\{~\kvd{let!}~y = c~\kvd{in}~\cexpr_1~\}~\kvd{in}~\cexpr_2 ~\} ~\equiv~\\
~\equiv~ \ident{asyncSeq}~\{~ \kvd{let!}~y = c~\kvd{in}~\kvd{let!}~x = \ident{async}~\{~\cexpr_1~\}~\kvd{in}~\cexpr_2 ~\}
\end{array}
\end{equation*}
%
The first equation returns the value $v$ without any asynchronous waiting in both cases
(although, in presence of side-effects, this is made more complicated by cancellation). 
The second equation is more subtle. The left-hand side awaits a single asynchronous workflow 
that first awaits $c$ and then does more work. The right-hand side awaits $c$ lifted to an 
asynchronous sequence and then awaits the rest (again, lifted into an asynchronous sequence).

% --------------------------------------------------------------------------------------------------

\subsection{Applicative computations}
\label{sec:laws-appl}

The last type of computations that we discussed \S\ref{sec:intro-formlets} is \emph{applicative functor}.
We use the less common definition (called \ident{Monoidal} by McBride and Paterson [99]). The 
definition consists of \emph{Map} and \emph{Merge} operations, together with a unit computation.
We use the unit to define \emph{Zero} -- in the translation it will only be used in computations that 
contain some unit-returning expression, such as $()$.

The identity law guarantees that merging with a unit and then projecting the non-unit value
produces an equivalent computation:
%
\begin{equation*}
\begin{array}{l}
\ident{f}~\{~  \kvd{let!}~x = \ident{f}~\{~()~\}\\
\hspace{1.4em} \kvd{and}~y = c~\kvd{in}~\kvd{return}~y~\}
\}
\end{array}
~\equiv~ c ~\equiv~
\begin{array}{l}
\ident{f}~\{~  \kvd{let!}~x = c\\
\hspace{1.4em} \kvd{and}~y = \ident{f}~\{~()~\}~\kvd{in}~\kvd{return}~x~\}
\}
\end{array}
\end{equation*}
%
The naturality law specifies that \emph{Merge} distributes over \emph{Map}, which translates to
the following equivalence (assuming $x_1$ not free in $\expr_2$ and vice versa):
%
\begin{equation*}
\begin{array}{c}
\begin{array}{l}
\ident{f}~\{~ \kvd{let!}~y_1 = \ident{f}~\{~\kvd{let!}~x_1 = c_1~\kvd{in}~\kvd{return}~\expr_1 ~\} \\
\hspace{1.40em}   \kvd{and}~y_2 = \ident{f}~\{~\kvd{let!}~x_2 = c_2~\kvd{in}~\kvd{return}~\expr_2 ~\}~\kvd{in}~ \expr~\}
\end{array} ~\equiv~
\\[1.2em]
~\equiv~
\begin{array}{l}
\ident{f}~\{~ \kvd{let!}~x_1 = c_1~\kvd{and}~x_2 = c_2~\kvd{in}~ 
              \kvd{let}~y_1, y_2 = \expr_1, \expr_2~\kvd{in}~ \expr~\}
\end{array}
\end{array}
\end{equation*}

As with the earlier syntactic rules, we can leave out the non-standard aspect
of the computations and read them as ordinary functional code and get correct and 
expected laws. This means that the laws, again, guarantee that intuition about the
syntax used by computation expressions will be correct.

Finally, the \emph{Merge} operation is also required to be associative -- this does not have
any corresponding syntax, but it means that the user does not need to know implementation
details of the compiler -- it does not matter whether \ident{let!}\ldots\ident{and}\ldots\,
is left-associative or right-associative.

% ==================================================================================================

\section{Conclusions}
\label{sec:conclusions}

Related work - do notation works for any monad - which makes it more reusable but weaker

By contrast, Haskell ``do" notation or Scala ``for" notation are relatively weak: they are limited
to a single kind of computation and they do not integrate exception handling or looping.

By contrast, the main computational notations in  Haskell (``do-notation'') and Scala (``flatMap monads``) 
are relatively weak: for example, they don't integrate with an exception-handling syntax, and they don't support the
range of computations described in this paper.  The observations in this paper are thus relevant to languages in the spirit of 
Haskell, Scala, C\#, F\#,  Javascript and Python. All of these have added (and sometimes deprecated) 
multiple non-trivial syntactic extensions aimed at improved computational expressivity -- queries, iterators, monadic syntax, 
comprehensions, asynchronous computations, futures, arrows, continuations, reactions -- with 
widespread impact on mainstream programming as a result. Importantly, the ``syntactic budget'' to support 
compositional computations in such languages is somewhat limited -- for example, Haskell and 
its extensions use four notations for  comprehensions, monads, idioms and arrows alone, and both C\# and Scala
have multiple notational devices for queries, comprehensions, iterators and asynchronous programming. 
The more ``bang'' we get with one syntactic mechanism, the better. As we show in this paper, computation expressions
give a lot of ``bang'' for relatively low cost.

% ==================================================================================================

\subsubsection{Acknowledgements.} We are grateful to Dominic Orchard, Alan Mycroft, Sam Lindley,
anonymous reviewers of previous draft and the audience of TFP 2012.


\bibliographystyle{abbrv}
\bibliography{computation-zoo}


\end{document}












