\documentclass{tmr}
\usepackage{mflogo}

%include lhs2TeX.fmt
%include lhs2TeX.sty
%include polycode.fmt

\title{Fun with Parallel Monad Comprehensions}
\author{Tomas Petricek\email{tomas.petricek@cl.cam.ac.uk}}

\begin{document}
\newcommand{\fsharp}[0]{F$^\#$}
\newcommand{\ident}[1]{{\normalfont\sffamily #1}}

% ==================================================================================================

\begin{introduction} 
Monad comprehensions have an interesting history. They were the first incarnation of monads in a 
programming language. Later, they were removed and replaced with just list comprehensions and 
the do notation. Now, monad comprehensions are back more powerful than ever before!

Redesigned monad comprehensions generalize the syntax for working with lists. Quite interestingly, 
they also generalize syntax for zipping, grouping and ordering of lists. This article, shows how to 
use some of the new expressive power of monad comprehensions when working with well-known monads. 
You'll learn what ``parallel composition'' means for parsers, poor man's concurrency
monad and an evaluation order monad.
\end{introduction}

% ==================================================================================================

\section{Introduction}

This article is inspired by my earlier work on \textit{joinads} \cite{joinads}, an extension that 
adds pattern matching on abstract values to the \textit{computation expression} syntax in \fsharp. 
Computation expressions are quite similar to the do notation in Haskell, but there are some notable 
differences. I'll briefly write about them later. After implementing the \fsharp \ version of 
joinads, I wanted to see how the ideas would look in Haskell. I was quite surprised to find out that 
a recent patch for GHC adds some of the expressive power of joinads to Haskell. 

To add some background -- the \fsharp \ computation expression syntax can be used to work with 
\textit{monads}, but also with \textit{monoids} and a few other abstract notions of computations. 
It also adds several constructs that make sense in an impure language including |while| and 
|for| loops as well as exception handling. The \textit{joinads} extension adds support for 
pattern-matching on ``monadic values''. For example, you can define a parallel programming monad
and use joinads to wait until two parallel computations both complete or wait until the first of 
the two completes returning a value matching particular pattern.

How are \fsharp \ joinads related to Haskell? A recent GHC patch implemented by Nils Schweinsberg 
\cite{bringbackmc} \cite{comprefun} brings back the support for monad comprehensions to Haskell. 
The patch doesn't just re-implement original monad comprehensions, but it also generalizes recent 
additions to list comprehensions. It makes it possible to use parallel \textit{monad} 
comprehensions and define monadic versions of operations like ordering and grouping 
\cite{groupordercompre}. 

The operation that generalizes parallel comprehensions is closely related to one operation that 
I designed for \fsharp \ joinads. In the rest of the article, I'll demonstrate some of the 
interesting things that can be done using this operation and the elegant syntax provided by 
re-designed monad comprehensions. 

% --------------------------------------------------------------------------------------------------

\subsection{Quick review of list comprehensions}
List comprehensions are a very powerful mechanism for working with lists in Haskell. I expect that 
you're already familiar with them, but let me start with a few examples. I'll use them later to 
demonstrate how the generalized monad comprehension syntax works in a few interesting cases.
If we have a list |animals| containing ``cat'' and ``dog'' and a list |sounds| containing 
``meow'' and ``woof'', we can write the following snippets:

\begin{verbatim}
> [ a ++ " " ++ s | a <- animals, s <- sounds ]
["cat meow","cat woof","dog meow","dog woof"]

> [ a ++ " " ++ s | a <- animals, s <- sounds, a !! 1 == s !! 1 ]
["dog woof"]

> [ a ++ " " ++ s | a <- animals | s <- sounds ]
["cat meow","dog woof"]
\end{verbatim}
The first example uses just the basic list comprehension syntax. It uses two \textit{generators}
to implement a join of the two collections. The second example adds a guard to specify that we
want only pairs of strings whose second character is the same. The guard serves as an additional
filtering of the results.

The last example uses parallel list comprehensions. The syntax is available after enabling the 
\ident{ParallelListComp} language extension. It allows us to take elements from
multiple lists in parallel, so that a n$^\text{th}$ element of the first list is matched with the 
n$^\text{th}$ element of the second list. The same functionality can be easily expressed using the 
|zip| function. 

% --------------------------------------------------------------------------------------------------

\subsection{Generalizing to monad comprehensions}
The three examples we've seen in the previous section are quite simple when working with lists.
When you apply the patch that implements monad comprehensions \cite{bringbackmc} and turn on the
\ident{MonadComprehensions} language extension, you can use the same syntactic sugar for working
with other types of computations. If we implement appropriate type classes, we can even use guards,
parallel comprehensions and operations like ordering or grouping. Figure~\ref{fig:monad-defs} shows 
the type classes and functions that are used by the desugaring.

\begin{figure}
\begin{verbatim}
class Monad m where
  (>>=)  :: m a -> (a -> m b) -> m b
  return :: a -> m a

class (Monad m) => MonadPlus m where
  mzero :: m a
  mplus :: m a -> m a -> m a

class (Monad m) => MonadZip m where
  mzip :: m a -> m b -> m (a, b)

guard :: MonadPlus m => Bool -> m ()
guard b = if b then return () else mzero
\end{verbatim}
\caption{Type-classes and functions used by monad comprehensions}
\label{fig:monad-defs}
\end{figure}

Aside from \ident{Monad}, the desugaring also uses \ident{MonadPlus} and \ident{MonadZip} 
type-classes. The former is used only for the |guard| function, which is defined in the 
figure. The latter is a new class that has been added as a generalization of parallel
list comprehensions. The name of the function makes it clear that the type-class is a generalization
of the |zip| function. The patch also defines a \ident{MonadGroup} type-class that generalizes 
grouping operations inside list comprehensions, but I will not use the feature in this article, 
so I omitted the class from the figure.

You can find the general desugaring rules in the patch description \cite{bringbackmc}. In this 
article, we'll just go through the examples from the previous section and examine how the 
translation looks. The following declaration shows the definition of the two operations required 
by \ident{Monad} for lists:

\begin{verbatim}
source >>= f = concat $ map f source
return a = [a]
\end{verbatim}
The bind operation calls the provided function for each element of the input list and then 
concatenates the generated lists. The |return| function creates a singleton list containing the
specified value. The |mzero| value from \ident{MonadPlus} type-class is an empty list, which 
means that |guard| returns |[()]| when the argument is |True| and empty list otherwise. Finally,
the |mzip| function for lists is just |zip|.

Now we have everything we need to look at the desugaring of monad comprehensions. The first
example from the previous section used multiple generators and can be translated purely in terms
of \ident{Monad}:

\begin{verbatim}
animals >>= (\a -> sounds >>= (\s -> 
    return $ a ++ " " ++ b))
\end{verbatim}
Every \textit{generator} is translated to a binding using |>>=|. The operations are nested and
the innermost operation always returns the result of the output function. The next snippet shows
what happens when we add predicate to filter the results:

\begin{verbatim}
animals >>= (\a -> sounds >>= (\s -> 
    guard (a !! 1 == s !! 1) >>= (\_ -> 
        return $ a ++ " " ++ s) ))
\end{verbatim}
A predicate is translated into a call to the |guard| function in the innermost part of the 
desugared expression. When the function returns |mzero| value (an empty list), the result of the
binding will also be |mzero|, so the element for which the predicate doesn't hold will be 
filtered out. Finally, let's look at the translation of the last example:

\begin{verbatim}
(animals `mzip` sounds) >>= (\(a, s) -> 
    return $ a ++ " " ++ s)
\end{verbatim}
When we use parallel comprehensions, the inputs of the generators are combined using the |mzip|
function. The result is then passed to binding, which calls the output function. If we also 
specified filtering, the |guard| function would be added to the innermost expression as in the 
previous example.

As you can see, the translation of monad comprehensions is quite simple, but it adds an additional
expressivity to the syntax. In particular, the do notation doesn't provide any equivalent for 
parallel comprehensions. Constructs like generalized ordering (using functions of type |m a -> m a|) 
and generalized grouping (using functions of type |m a -> m (m a)|) add even more expressivity, but 
that would be a topic for another article. In the next three sections, I'll show how we could 
implement the |mzip| operation for several interesting monads, starting with monadic parsers.

% ==================================================================================================

\section{Composing parsers in parallel}
What does a \textit{parallel composition of two parsers} mean? Probably the best thing we can do 
is to run both parsers on the input string and return a tuple with the two results. That sounds
quite simple, but what is this construct good for? Let's first implement it and then look at 
some examples. 

% --------------------------------------------------------------------------------------------------

\subsection{Introducing parsers}
Many of you may already know monadic parsing combinators. I include all definitions that are 
necessary to build a complete example for two reasons -- to make this article self-contained
and because there is one minor, but important twist.

A parser is a function that takes the 
input string and returns a list of possible results. It may be empty (if the parser fails) or 
contain several items (if there are multiple ways to parse the input). The implementation I use 
in this article mostly follows the one by Hutton and Meijer \cite{monadparsing}:

\begin{verbatim}
newtype Parser a
  = Parser (String -> [(a, Int, String)])
\end{verbatim}
The result of parsing is a tuple containing a value of type |a| produced by the parser, a number
of characters consumed by the parser and the remaining unparsed part of the string. The |Int| value
is not usually included in parser definitions. We'll need it later in the implementation of |zip|.
As you'll see, when running parsers in parallel, we can only combine results if they consumed the
same number of characters.

Now that we have a definition of parsers, we can create our first primitive parser and a 
function that runs parser on an input string and returns the results:

\begin{verbatim}
item :: Parser Char
item = Parser (\input -> case input of
  "" -> []
  c:cs -> [(c, 1, cs)])
  
run :: Parser a -> [a]
run (Parser p) input = 
  [ result | (result, _, tail) <- p input, tail == [] ]
\end{verbatim}
The |item| parser returns the first character of the input string. When it succeeds, it consumes 
just a single character, so it returns 1 as the second element of the tuple. The |run| function
applies the underlying function to a specified input and then returns results that completely 
parsed the whole input string as specified by the condition |tail == []|. The next step is to make
the parser monadic.

% --------------------------------------------------------------------------------------------------

\subsection{Implementing the parser monad}
Parsers are a well known example of \textit{monads} and \textit{monoids}. This means that we can 
implement both \ident{Monad} and \ident{MonadPlus} type classes. You
can find the implementation in Figure~\ref{fig:monad-instance}.

\begin{figure}
\begin{verbatim}
instance Monad Parser where
  return a = Parser (\input -> [(a, 0, input)])
  (Parser p1) >>= f = Parser (\input ->
    [ (result, n1 + n2, tail) 
        | (a, n1, input') <- p1 input
        , let (Parser p2) = f a
        , (result, n2, tail) <- p2 input' ])

instance MonadPlus Parser where 
  mzero = Parser (\_ -> [])
  mplus (Parser p1) (Parser p2) = Parser (\input ->
    p1 input ++ p2 input)
\end{verbatim}
\caption{Instances of \ident{Monad} and \ident{MonadPlus} for parsers}
\label{fig:monad-instance}
\end{figure}

The |return| operation returns a single result containing the specified value that doesn't consume
any input. The |>>=| operation can be implemented using ordinary list comprehensions.
It runs the parsers in sequence, returns the result of the second parser and consumes the sum of 
characters consumed by the first and the second parser. The |mzero| operation creates a parser that 
always fail and |mplus| represents a non-deterministic
choice between two parsers. 

The two type class instances allow us to use some of the monad comprehension syntax. We can now 
use the |item| primitive to write a few simple parsers:

\begin{verbatim}
sat pred = [ ch | ch <- item, pred ch ]
char ch = sat (ch ==)
notChar ch = sat (ch /=)

some p = [ a:as | a <- p, as <- many p ]
many p = some p `mplus` return []
\end{verbatim}
The |sat| function creates a parser that parses a character matching a specified predicate. 
The \textit{generator syntax} |c <- item| corresponds to monadic binding and is desugared using 
the |>>=| operation. The expression specifying the result (in our case |ch|) is returned using
the monadic |return|. When a type implements the \ident{MonadPlus} type class, 
we can also use \textit{guards} such as |pred ch|. When the predicate doesn't hold, the returned 
parser is created using |mzero|, which represents a failure.

The |some| combinator creates a parser that expects one or more occurrences of |p|. We encode it
using monad comprehension with two bindings. The parser parses |p| followed by |many p|. The same 
thing could be written using do notation, but monad comprehensions provide quite elegant and 
succinct alternative, with a syntactic support for guards (as you can see in the first example).

The monad comprehension syntax makes it maybe slightly less obvious than the do notation, but the 
order of bindings matters. This is important for the next example, which gets the body of 
an expression enclosed between brackets:

\begin{verbatim}
brackets :: Char -> Char -> Parser a -> Parser a
brackets op cl body = 
  [ inner 
      | _ <- char op
      , inner <- brackets op cl `mplus` body
      , _ <- char cl ]

skipBrackets = brackets '(' ')' (many item)
\end{verbatim}
The combinator takes characters representing opening and closing brackets and a parser for parsing 
the body inside brackets. It uses a monad comprehension with three binding expressions that parse
opening brace, the body or more brackets and then the closing brace.

If you run the parser using |run skipBrackets "((42))"| you get a list containing |"42"|, but also 
|"(42)"|. This is because the |many item| parser can also consume brackets. To correct that, we need
to write a parser that accepts any character except opening and closing brace. As you'll see 
shortly, this can be elegantly solved using parallel comprehensions.

% --------------------------------------------------------------------------------------------------

\subsection{Parallel composition of parsers}
To support parallel monad comprehensions, we need to implement \ident{MonadZip}. 
As a reminder, the type class defines an operation |mzip| with the following type:

\begin{verbatim}
mzip :: m a -> m b -> m (a, b)
\end{verbatim}
By looking just at the type signature, you can see that the operation can be implemented in terms
of |>>=| and |return|. This is a reasonable definition for some monads, such as the |Reader| monad 
[\textit{sic}], but not for all of them. For example, |mzip| for lists uses a different operation. 
The implementation for parsers in Figure~\ref{fig:monadzip-instance} also isn't expressed using
other monad primitives.

\begin{figure}
\begin{verbatim}
instance MonadZip Parser where 
  mzip (Parser p1) (Parser p2) = Parser (\input -> 
    [ ((a, b), n1, tail1) 
        | (a, n1, tail1) <- p1 input
        , (b, n2, tail2) <- p2 input
        , n1 == n2 ])
\end{verbatim}
\caption{Instance of \ident{MonadZip} type class for parsers}
\label{fig:monadzip-instance}
\end{figure}

The parser created by |mzip| independently parses the input string using both of the parsers.
It uses list comprehensions to find all combinations of results such that the number of 
characters consumed by the two parsers was the same. For each matching combination, the parser 
returns a tuple with the two parsing results. Requiring that the two parsers consume the same 
number of characters is not an arbitrary decision. It means that the remaining unconsumed strings 
|tail1| and |tail2| are the same and so we can return any of them. I use a counter instead of 
comparing strings, because it allows us to work with possibly infinite strings.

Let's get back to the example with parsing brackets. The following snippet uses parallel 
monad comprehensions to create a version that consumes all brackets:

\begin{verbatim}
skipAllBrackets = brackets '(' ')' body
  where body = many [ c | c <- notChar '(' | _ <- notChar ')' ]
\end{verbatim}
The parser |body| takes zero or more of any characters that are not opening or closing brackets.
The parallel comprehension runs two |notChar| parsers on the same input. They both read a single
character and they succeed if the character is not `(' and `)' respectively. The resulting parser
succeeds only if both them succeed. Both of the parallel parsers read the same char, so we 
assign the first one to a symbol |c| and return it as the result and ignore the second one.

Another example where this syntax may be useful is validation of inputs. For example, a Cambridge
phone number consists of 10 symbols, needs to contain only digits and starts with 1223. The new
syntax allows us to directly encode these three rules:

\begin{verbatim}
cambridgePhone = 
  [ n | n <- many (sat isDigit)
      | _ <- times item 9 
      | _ <- startsWith (string "1223") ]
\end{verbatim}
The encoding is quite straightforward. We need some additional combinators such as |times| that
repeats a parser specified number of times and |startsWith| that runs a parser and then consumes
any number of characters. 

We could construct a single parser that recognizes valid Cambridge phone number without using
|mzip|. The point of this example is that we can quite nicely combine several independent rules,
which makes the validation code easy to understand and extend.

% --------------------------------------------------------------------------------------------------

\subsection{Applicative and parallel parsers}

Monadic parser combinators are very expressive. In fact, they are often \textit{too expressive}, 
which makes it difficult to implement the combinators efficiently. This was a motivation for
the development of non-monadic parsers, such as the one by Swierstra \cite{deterparser}, 
\cite{parsingtutorial}. They are less expressive, but can be more efficient.  

A weaker interface that can be used for writing parsers is \textit{applicative functor}
\cite{applicative} developed by McBride and Paterson. Intuitively, applicative functors allow
us to write parsers where the choice of the next parser doesn't depend on the value parsed 
so far. In terms of formal language theory, they can express only \textit{context-free} languages.
This is still sufficient for many practical purposes. For example our earlier |brackets| parser
can be written using the applicative combinators:

\begin{verbatim}
brackets op cl body = 
  pure (\_ inner _ -> inner)
    <*> char op
    <*> brackets op cl body `mplus` body
    <*> char cl
\end{verbatim}
The example creates a parser that always succeeds and returns a function using the |pure| 
combinator. Then it applies this function (contained in a parser) to three arguments (produced
by the three parsers). You don't need to understand the example in details. The important
point is -- if we have a comprehension where the next \textit{generator} in a sequence
doesn't depend on the value produced by the previous one, we can rewrite it using the 
\textit{applicative} interface. For parsers, this means that we're parsing just a 
\textit{context-free grammar}.

The interesting question is, what operation does |mzip| represent for context-free grammars?
A language we obtain if parses for two other languages both succeed is an \textit{intersection} 
of the two languages. An intersection of two context-free languages is not context-free, which
can be demonstrated using the following example:

\[ 
A = \{ a^m b^m c^n \arrowvert m, n \geq 0 \} 
\qquad
B = \{ a^n b^m c^m \arrowvert m, n \geq 0 \} 
\]
\[ 
A \cap B = \{ a^m b^m c^m \arrowvert m \geq 0 \}
\]
The language $A$ accepts words that start with some number of `a' followed by the same
number of `b' and then arbitrary number of `c' symbols. The language $B$ is similar, but it
starts with a group of any length followed by two groups of the same lengths. Both of the
languages are context-free. In fact, our parser |brackets| can be used to parse the two 
character groups with same lengths.

The intersection $A \cap B$ is not context-free \cite{cflintersect}, but we can easily
encode it using parallel composition of parsers. We don't need the full power of monads
to parse the first group and calculate its length. We can simply write an intersection 
of two parsers:

\begin{verbatim}
[ True | _ <- many $ char 'a', _ <- brackets 'b' 'c' unit
       | _ <- brackets 'a' 'b' unit, _ <- many $ char 'c' ]
  where unit = return ()
\end{verbatim}
The example uses both parallel and sequential binding, but the sequential composition doesn't
contain dependencies on previously parsed results. It uses |brackets| to parse two groups of the
same length, followed (or preceded) by |many| to consume the remaining group of arbitrary length.

Before moving to a next example, I should mention how |mzip| relates to applicative parsers.
An alternative definition of applicative functor (\cite{applicative} section 7) uses an operation
$\star$ with exactly the same type signature as |mzip|. Since applicative functors are weaker than 
monads, we can always define $\star$ in terms of |>>=| and |return| and get \textit{some} 
implementation of |mzip|.

This definition may not always be intuitively right. Referring to the intuition is always tricky, 
so I'll try to express my expectations in terms of laws at the end of the article. For now, we 
can look at the example of lists. The definition of |mzip| using the \ident{List}
monad would give us a cartesian product of lists. This doesn't look very useful. Instead, list 
comprehensions use zipping of lists, which comes from a different applicative functor, namely
\ident{ZipList}.

The example with parsers that I presented is quite similar. The implementation of |mzip| using 
monad primitives would give us a sequential composition of parsers. This wouldn't be very useful,
so I defined |mzip| as the intersection of parsers. This is an interesting operation that could be 
used to define a different instance of applicative functor for parsers.

% ==================================================================================================

\section{Parallelizing cooperative computations}

As the name \textit{parallel} monad comprehensions suggests, we can use the syntax for 
running computations in parallel. Unlike comprehensions with multiple generators, parallel
comprehensions cannot have any dependencies between the composed bindings. This means that the 
computations can be evaluated independently. 

In this section, I demonstrate the idea using a poor man's concurrency monad inspired by Claessen's
work \cite{poorman}. The monad can be used to implement a lightweight cooperative concurrency. When 
running two computations in parallel, we can allow interleaving of atomic actions from the two 
threads.

% --------------------------------------------------------------------------------------------------

\subsection{Modelling resumable computations}

The example I demonstrate here models computations using \textit{resumptions}. This concept is 
slightly simpler than the original poor man's concurrency monad (which is based on continuations). 
A resumption is a computation that has either finished and produced some value or has not
finished, in which case it can run one atomic step and produce a new resumption:

\begin{verbatim}
data Monad m => Resumption m r 
  = Step (m (Resumption m r))
  | Done r
\end{verbatim}
The type is parametrized over a monad and a return type. When evaluating a resumption, we 
repeatedly run the computation step-by-step. While evaluating, we perform the effects allowed by
the monad |m| until we eventually get a result of type |r|. If you're interested in more details 
about the |Resumption| type, you can find a similar definition in Harrison's cheap threads 
\cite{cheapthreads} in Papaspyrou's resumption transformer \cite{resmonad}.

Now that we have a type, we can write a function to create a |Resumption| that runs a single
atomic action and then completes and a function that runs a |Resumption|:

\begin{verbatim}
run :: Monad m => Resumption m r -> m r
run (Done r) = return r
run (Step m) = m >>= run

action :: Monad m => m r -> Resumption m r
action a = Step [ Done r | r <- a ]
\end{verbatim}
The |run| function takes a resumption that may perform effects specified by the monad |m| and it
runs the resumption inside the monad until it reaches |Done|. The function runs the whole 
computation sequentially (and it cannot be implemented differently). The cooperative concurrency
can be added later by creating a combinator that interleaves the steps of two |Resumption|
computations.

The |action| function is quite simple. It returns a |Step| that runs the specified action 
inside the monad |m| and then wraps the result inside the |Done| constructor. I implemented the
function using monad comprehensions, but we could also use the usual do notation, which may be
a better fit for effectful computations:

\begin{verbatim}
action' a = Step $ do 
  r <- a
  return (Done r)
\end{verbatim}
The function does exactly the same thing as the previous version. The comprehension syntax was 
slightly more succinct, because the computation ends with |return|, which is the standard structure
of comprehensions.

% --------------------------------------------------------------------------------------------------

\begin{figure}
\begin{verbatim}
instance Monad m => Monad (Resumption m) where
  return a = Done a
  (Done r) >>= f = Step $ return (f r)
  (Step s) >>= f = Step $ do
    next <- s
    return $ next >>= f

instance MonadTrans Resumption where
  lift = action
\end{verbatim}
\caption{Instances of \ident{Monad} and \ident{MonadTrans} 
  for resumptions.}
\label{fig:poorman-instance}
\end{figure}

\subsection{Implementing the resumption monad}
You can see the implementation of the \ident{Monad} type class in 
Figure~\ref{fig:poorman-instance}. The |return| operation creates a new resumption that is finished
and contains the specified value. The |>>=| operation constructs a resumption that gets the result
of the first resumption and then calls the function |f|. When the left parameter is |Done|, we 
apply the function to the result and wrap the application inside |return| (because the function
is pure) and |Step|. When the left parameter is |Step|, we create a resumption that runs the
step and then uses |>>=| recursively to continue running steps from the left argument until it 
finishes.

The listing also defines an instance of the \ident{MonadTrans} type class to make
|Resumption| a monad transformer. The |lift| function takes a computation in the monad |m| and turns
it into a computation in the |Resumption| monad. This is exactly what our function for wrapping 
atomic actions does.

Equipped with the two type class instances and the |run| function, we can write some interesting
computations. The following function creates a computation that runs for the specified number of
steps, prints some string in each step and then returns a specified value:

\begin{verbatim}
printLoop :: String -> Int -> a -> Resumption IO a
printLoop str count result = do
  lift $ putStrLn str
  if count == 1 then return result
  else printLoop str (count - 1) result

cats = run $ printLoop "meow" 3 "cat"
\end{verbatim}

The function is written using the do notation. It first prints the specified string, using 
|lift| to turn the |IO ()| action into a single-step |Resumption IO ()|. When the counter reaches
one, it returns the result and otherwise it continues looping.

The snippet also defines a simple computation |cats| that prints ``meow'' three times and then 
returns a string ``cat''. The interesting aspect of computations created by |printLoop| is that 
they are not fully opaque. If we have two computations like that, we can treat them as sequences
of steps and interleave them. This is what the |mzip| operation does.

% --------------------------------------------------------------------------------------------------

\subsection{Parallel composition of resumptions}

If we have two resumptions, we can compose them to run in sequence either using the do notation or
using a monad comprehension with two generators. To run them in parallel, we need to implement
interleaving of the steps as shown in Figure~\ref{fig:poorman-zip}. 

\begin{figure}
\begin{verbatim}
instance Monad m => MonadZip (Resumption m) where
  mzip (Done a) (Done b) = Done (a, b)
  mzip sa sb = Step [ mzip a b | a <- step sa, b <- step sb ]
    where step (Done r) = return $ Done r 
          step (Step sa) = sa
\end{verbatim}
\caption{Instance of \ident{MonadZip} that composes resumptions in parallel}
\label{fig:poorman-zip}
\end{figure}

The result of |mzip| is a resumption that consists of multiple steps. In each step, it performs
one step of both of the resumptions given as arguments. When it reaches a state when both of the
resumptions complete and produce results, it returns a tuple containing the results using |Done|.
A step is performed using an effectful |step| function. To keep the implementation simple, we keep
applying |step| to both of the resumptions and then recursively combine the results. Applying
|step| to a resumption that has already completed isn't a mistake. This operation doesn't do 
anything and just returns the original resumption (without performing any effects).

Once we define |mzip|, we can start using the parallel comprehension syntax for working with 
resumptions. The next snippet demonstrates two ways of composing resumptions. In both examples, we 
compose a computation that prints ``meow'' two times and then returns ``cat'' with a computation 
that prints ``woof'' three times and then returns ``dog'':

\begin{verbatim}
animalsSeq = 
  [ c ++ " and " ++ d
     | c <- printLoop "meow" 2 "cat" 
     , d <- printLoop "woof" 3 "dog" ]

animalsPar = 
  [ c ++ " and " ++ d
     | c <- printLoop "meow" 2 "cat" 
     | d <- printLoop "woof" 3 "dog" ]
\end{verbatim}

The only difference between the two examples is that the first one composes the operations using 
multiple generators (separated by comma) and the second one uses parallel comprehensions (separated
by bar).

When you run the first example, the program prints meow, meow, woof, woof, woof and then returns 
a string ``cat and dog''. The second program interleaves the steps of the two computations and 
prints meow, woof, meow, woof, woof and then returns the same string.	

% ==================================================================================================

\section{Composing computations in parallel}

In the previous section, we used the parallel comprehension syntax to create computations that 
model parallelism using resumptions. Resumptions can be viewed as lightweight cooperative threads.
They are useful abstraction, but they do not give us any speed-up on multi-core CPU. This section
will follow a similar approach, but we look how to implement actual parallelism based on 
\textit{evaluation strategies}. 

Marlow et al. \cite{strategies-new} introduced an \ident{Eval} monad for explicitly 
specifying the evaluation order. I start the section by briefly introducing the monad, so you don't 
need to worry if you're not familiar with it already. Then I'll demonstrate how to define a 
\ident{MonadZip} instance for this monad. This way, we can use the parallel 
comprehension syntax for running computations actually in parallel.

% --------------------------------------------------------------------------------------------------

\subsection{Introducing evaluation-order monad}

The evaluation-order monad is represented by a type |Eval|. When writing code inside the monad,
we can use several functions of type |a -> Eval a| that are called \textit{strategies}. A strategy
takes some computation and wraps it inside the monad. It can specify evaluation strategy for the
computation. For example |rpar| starts evaluating the value of the argument in background and
|rseq| evaluates the value eagerly before returning. 

A typical pattern is to use the do notation to spawn one computation in parallel and then 
run another computation sequentially. This way we can easily parallelize two function calls:

\begin{verbatim}
fib38 = runEval $ do 
  a <- rpar $ fib 36
  b <- rseq $ fib 37
  return $ a + b
\end{verbatim}

The example shows how to calculate 38$^\text{th}$. Fibonacci number. It starts calculating 
|fib 36| in parallel with the rest of the continuation and then calculates 
|fib 37| sequentially. The do notation creates a value of type |Eval Integer|. 
We can then pass the value to |runEval|, which returns the wrapped value. Because we used the
|rpar| and |rseq| combinators when constructing the computation, the returned value will 
be already evaluated.

However, it is worth noting that the |return| operation of the monad doesn't specify any evaluation 
order. The function |runEval . return| is just an identity function that doesn't force evaluation 
of the argument. The evaluation order is specified by additional combinators such as |rpar|.

The \ident{Eval} monad is defined in the \ident{parallel} package
\cite{parallelpkg}. You can find it's definition in the Figure~\ref{fig:eval-monad}. The |rpar| 
and |rseq| combinators are not shown (they can be easily implemented using the |par| and |pseq| 
annotations). 

\begin{figure}
\begin{verbatim}
data Eval a = Done a

runEval :: Eval a -> a
runEval (Done x) = x

instance Monad Eval where
  return x = Done x
  Done x >>= k = k x
\end{verbatim}
\caption{ Evaluation-order monad \ident{Eval} with a 
          \ident{Monad} instance }
\label{fig:eval-monad}
\end{figure}

The |Eval a| type is simple. It just wraps a value of type |a|. The |runEval| function unwraps the 
value and the \ident{Monad} instance implements composition of computations in the 
usual way. The power of the monad comes from the annotations we can add. When using the monad, 
we're transforming values of type |a| into values of type |Eval a| and we can specify the 
evaluation strategy. The strategy is usually specified using combinators, but if we add an instance 
of the \ident{MonadZip} class, we can also specify the evaluation order using the
monad comprehension syntax.

% --------------------------------------------------------------------------------------------------

\subsection{Specifying parallel evaluation order}

The implementation of the |mzip| operator for the evaluation order monad simply extracts the 
common pattern that I demonstrated in the previous example. When composing two computations, we
wrap the first one using the |rpar| combinator and the second one using the |rseq| combinator.
You can see the implementation in Figure~\ref{fig:eval-zip}.

\begin{figure}
\begin{verbatim}
instance MonadZip Eval where
  mzip ea eb = do
    a <- rpar $ runEval ea
    b <- rseq $ runEval eb
    return (a, b)
\end{verbatim}
\caption{Instance of \ident{MonadZip} for parallelizing tasks}
\label{fig:eval-zip}
\end{figure}

A slightly tricky aspect of the |Eval| type is that they may represent computations with explicitly
specified evaluation order (created, for example, using |rpar|) as well as computations without 
associated evaluation order (created using |return|). This makes it possible to implement the 
|mzip| function. The arguments of the function have types |Eval a| and |Eval b|, but we can assume 
that they don't come with evaluation order specification. 

The implementation of |mzip| extracts the underlying (unevaluated) values using |runEval|, 
specifies the evaluation order using |rpar| and |rseq| and then return a result of type 
|Eval (a, b)|, which now carries the evaluation order specification. 

Let's look how we can write a sequential and parallel version of a snippet that calculates
38$^\text{th}$ Fibonacci number:

\begin{verbatim}
fibTask n = return $ fib n

fib38seq = runEval [ a + b | a <- fibTask 36
                           , b <- fibTask 37 ]
fib38par = runEval [ a + b | a <- fibTask 36
                           | b <- fibTask 37 ]
\end{verbatim}

The snippet first declares a helper function |fibTask| that creates a delayed value using 
the sequential |fib| function and wraps it inside the |Eval| monad without specifying evaluation
strategy. Then we can use the function as a source for generators in the monad comprehension
syntax. The first example runs the entire computation sequentially -- aside from some wrapping
and unwrapping, there are no evaluation order specifications. The second example runs the 
two sub-computations in parallel. The evaluation order annotations are added by the |mzip| 
function that is called thanks to the \textit{parallel comprehension} syntax.

If you compile the code using GHC with the |-threaded| option and run the program on multiple 
threads (using |+RTS -N2 -RTS| option), you should see that the second version runs faster. On my 
dual-core Intel Core 2 Duo CPU (2.26GHz), the time needed to run the first one is approximately
13 seconds and the second completes in about 9 seconds. 

\subsection{Writing parallel algorithms}
The parallel version of the calculation in the previous section cannot be two times faster, because 
it parallelizes just two computations that do not take equally long time. To generate better 
potential for parallelism, we can implement a recursive |pfib| function that splits the computation 
into two parallel branches recursively until it reaches some threshold:

\begin{verbatim}
pfib :: Integer -> Eval Integer
pfib n | n <= 35 = return $ fib n
pfib n = [ a + b | a <- pfib $ n - 1 
                 | b <- pfib $ n - 2 ]
\end{verbatim}
I hope you'll agree that the declaration using parallel comprehensions looks quite neat. A nice
consequence of using parallel comprehensions is that we can quite clearly see which parts of 
the computation will run in parallel, but there is no syntactic noise. We just need to replace 
a comma with a bar symbol to get a parallel version! The compiler also prevents us from trying to 
parallelize code that cannot run in parallel, because there is some data dependency. For example,
let's look at the Ackermann function: 

\begin{verbatim}
ack :: Integer -> Integer -> Eval Integer
ack 0 n = return $ n + 1
ack m 0 = ack (m - 1) 1
ack m n = [ a | na <- ack m (n - 1)
              , a <- ack (m - 1) na ]
\end{verbatim}
The Ackermann function is a well-known function from computability theory. It is interesting, 
because it grows really fast (as a result, it cannot be expressed using primitive form of 
recursion). For example, the value of |ack 4 2| is $2^{65536} - 3$. 

We're probably not going to be able to finish the calculation, no matter how many cores our CPU 
has. However, we can still try to parallelize the function by replacing the two sequential 
generators with parallel comprehension:

\begin{verbatim}
ack m n = [ a | na <- ack m (n - 1)
              | a <- ack (m - 1) na ]
\end{verbatim}
If you try compiling this snippet, you get an error message saying ``Not in scope: na''. The 
problem is that our second generator uses the value |na| which is calculated by the first generator.
This means that there is a data dependency and we cannot start the second computation before the
first one completes. 

The nice thing about using parallel comprehensions is that we get an error 
message. If you wrote the same snippet using the do notation, you could add evaluation order 
annotations using |rpar| and |rseq|. This would compile, but you wouldn't introduce any parallelism,
because the function is purely sequential.

% ==================================================================================================

\section{Parallel comprehension laws}
When discussing the implementation of |mzip| for parsers, I referred to the intuition. The type of 
|mzip| partially specifies how the operation should behave, but it is still very flexible. To 
express some of the intuition about \textit{parallel binding}, we can specify laws that should hold 
about |mzip|.

I intentionally postponed the discussion about laws to the end of the article. To my understanding, 
the laws about |mzip| are still subject to discussion. I first discuss the laws  that have been 
proposed in the patch discussion and then look at some laws based on my work
on \fsharp \ joinads \cite{joinads}.

\subsection{Current parallel binding laws}
The discussion about the patch \cite{bringbackmc} identifies two laws that should hold about 
|mzip| with respect to the |map| function of monads (the function can be expressed in 
terms of bind and return and corresponds to |liftM| from Haskell base library). The first law is 
called \textit{naturality} and the second one is \textit{information preservation}:

\begin{equation}
  \text{map} \: (f \times g) \: (\text{mzip} \: a \: b) \equiv \text{mzip} \: (\text{map} \: f \: a) \: (\text{map} \: g \: b)
  \label{naturality}
\end{equation}
\begin{equation}
  \text{map} \: \text{fst} \: (\text{mzip} \: a \: b) \equiv a \equiv \text{map} \: \text{snd} \: (\text{mzip} \: b \: a)
  \label{ipreserve}
\end{equation}
The naturality law (\ref{naturality}) specifies that we can change the order of applying 
|mzip| and |map|. This is quite common law that is also required by \textit{applicative functors}.
\cite{applicative}. The information preservation law (\ref{ipreserve}) specifies that adding 
|mzip| and then recovering the original form of the value using |map| doesn't lose 
information. This laws is a bit tricky though. For example, it doesn't hold for lists if $a$ and $b$ 
are lists of different length. The |zip| function restricts the length of the result to the length 
of the shorter list. Similarly, it doesn't hold for parsers (from the first section) that consume 
different number of characters. 

However, the law definitely expresses an important requirement. A similar law holds about applicative 
functors, but with an slight twist. Instead of zipping two arbitrary monadic values, the law zips 
an arbitrary value with a special value called |unit|. For lists, the |unit| value 
returns an infinite list, so the law holds. 

Ideally, we'd like to say that information should be preserved when the two monadic values have 
the \textit{same structure}. There is no direct way to refer to the ``structure'' of a monadic 
value, but we can create such values using the |map| function. The following weaker law 
expresses the requirement and holds for lists as well as parsers. In the equation, $f$ and $g$ are 
arbitrary (complete) functions:

\begin{equation}
  \text{map} \: \text{fst} \: (\text{mzip} \: a \: (\text{map} f a)) \equiv a \equiv \text{map} \: \text{snd} \: (\text{mzip}\: (\text{map} \: g \: a) \: a)
  \label{weakipres}
\end{equation}
This \textit{weak information preservation} law (\ref{weakipres}) is quite similar to 
(\ref{ipreserve}). The only difference is that instead of arbitrary monadic value $b$, 
we're zipping $a$ with a value constructed using \textit{map a}. This means that 
the actual value(s) that the monadic value contains (or produces) can be different, but the 
structure will be the same, because |map| preserves the structure. 

\subsection{Parallel binding laws of joinads}
As I already mentioned, an operation similar to |mzip| exists in \fsharp \ extension called joinads.
The operation requires a stronger set of laws, but it is used for a very similar purpose. Therefore
it may make sense to adopt some of the laws of joinads for |mzip|. There are two laws that specify
how the |mzip| operation interacts with |mzero| and |return|:

\begin{equation}
  \text{map} \: \text{fst} \: (\text{mzip} \: a \: \text{mzero}) \equiv a \equiv \text{map} \: \text{snd} \: (\text{mzip} \: \text{mzero} \: a)
  \label{zero}
\end{equation}
\begin{equation}
  \text{mzip} \: (\text{return} \: a) \: (\text{return} \: b) \equiv \text{return} \: (\text{mzip} \: a \: b)
  \label{unitmerge}
\end{equation}
The first law (\ref{zero}) specifies that |mzero| is the \textit{zero element} with respect to 
|mzip|. It looks almost necessary, because |mzero| with type |m a| doesn't contain any value of 
type |a|, so there is no way we could construct a value of type |(a, b)|. The law complements the 
\textit{weak information preservation} law (\ref{ipreserve}), but it would contradict with the 
original \textit{information preservation} 

The second law (\ref{unitmerge}) defines how |mzip| works with respect to the monadic |return|
operation. This is quite simple requirement, but it is possible to imagine some implementations 
for which it doesn't hold. The law isn't completely arbitrary. An equation with a similar structure
is required for \textit{causal commutative arrows} \cite{causalarr}.

Finally, the last two law required by joinads are a form of associativity and commutativity. The 
laws are expressed using two helper functions $assoc((a,b),c)=(a,(b,c))$ and $swap(a,b)=(b,a)$:

\begin{equation}
  \text{mzip} \: a \: (\text{mzip} \: b \: c) \equiv \text{map} \: \text{assoc} \: (\text{mzip} \: (\text{mzip} \: a \: b) \: c) 
  \label{commut}
\end{equation}
\begin{equation}
  \text{mzip} \: a \: b \equiv \text{map} \: \text{swap} \: (\text{mzip} \: a \: b)
  \label{assoc}
\end{equation}

\UndefineShortVerb{\|}
\DefineShortVerb{\_}

If we rewrite the \textit{commutativity} law (\ref{commut}) using the monad comprehension syntax, it 
specifies that _[(x, y) | x <- a | y <- b]_ should mean the same thing as _[(x, y) | y <- b | x <- a]_. 
This may look like a very strong requirement, but it fits quite well with the usual intuition about 
the |zip| operation. The commutativity law holds for lists, parsers and the evaluation order monad.
For poor man's concurrency monad, it holds if we treat effects that occur within a single step 
of the evaluation as unordered (which may be a reasonable interpretation).

The \textit{associativity} law (\ref{assoc}) may be even more desirable. When we write a 
comprehension such as _[e | a <- m1 | b <- m2 | c <- m3]_, the desugaring first needs to zip some
of the three inputs and then add the third one, because _mzip_ is a binary operation. The order
of zipping feels like an implementation detail, but if we don't require associativity, it may 
affect the meaning of our code.

\UndefineShortVerb{\_}
\DefineShortVerb{\|}

% ==================================================================================================

\section{Conclusions}
After some time, monad comprehensions are back in Haskell! The recent GHC patch makes them even 
more useful by generalizing additional features of list comprehensions including parallel binding 
and the support for operations like ordering and grouping. In this article, I focused on the first 
generalization, but the remaining two are equally interesting.

In this article, we looked at three examples: parallel composition of parsers (which applies 
parsers on the same input), parallel composition of monads for cooperative concurrency (which interleaves
the steps of computations) and a parallel composition of evaluation order monads (which runs two 
computations in parallel). Some of the examples are inspired by my previous work on joinads that add 
similar language extension to \fsharp. The \fsharp \ version includes an operation very similar to 
|mzip| from the added \ident{MonadZip} type class. I explained the laws about the \fsharp \ version of 
the operation, hoping that it may contribute to the discussion about the laws required by 
\ident{MonadZip}.

\bibliography{CompreFun}

\end{document}
























